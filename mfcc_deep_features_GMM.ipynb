{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file consist the process of formatting of datasets, the creation of npy files from the formatted dataset, then taking the mfcc of the data in the npy file and then finally using the final data to train the  model and same for all dev set as well except the mfcc and the training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package              Version             \n",
      "-------------------- --------------------\n",
      "absl-py              0.7.1               \n",
      "astor                0.8.0               \n",
      "attrs                19.1.0              \n",
      "audioread            2.1.8               \n",
      "backcall             0.1.0               \n",
      "bleach               3.1.0               \n",
      "blitz                0.3.3               \n",
      "blockdiag            1.5.4               \n",
      "bob.extension        3.1.1               \n",
      "certifi              2019.6.16           \n",
      "cffi                 1.12.3              \n",
      "chardet              3.0.4               \n",
      "Click                7.0                 \n",
      "click-plugins        1.1.1               \n",
      "cycler               0.10.0              \n",
      "decorator            4.4.0               \n",
      "defusedxml           0.6.0               \n",
      "entrypoints          0.3                 \n",
      "funcparserlib        0.3.6               \n",
      "gast                 0.2.2               \n",
      "google-pasta         0.1.7               \n",
      "grpcio               1.23.0              \n",
      "h5py                 2.9.0               \n",
      "idna                 2.8                 \n",
      "imbalanced-learn     0.5.0               \n",
      "ipykernel            5.1.2               \n",
      "ipython              7.7.0               \n",
      "ipython-genutils     0.2.0               \n",
      "ipywidgets           7.5.1               \n",
      "jedi                 0.15.1              \n",
      "Jinja2               2.10.1              \n",
      "joblib               0.13.2              \n",
      "jsonschema           3.0.2               \n",
      "jupyter              1.0.0               \n",
      "jupyter-client       5.3.1               \n",
      "jupyter-console      6.0.0               \n",
      "jupyter-core         4.5.0               \n",
      "Keras                2.2.5               \n",
      "Keras-Applications   1.0.8               \n",
      "Keras-Preprocessing  1.1.0               \n",
      "kiwisolver           1.1.0               \n",
      "librosa              0.7.0               \n",
      "llvmlite             0.29.0              \n",
      "Markdown             3.1.1               \n",
      "MarkupSafe           1.1.1               \n",
      "matplotlib           3.0.3               \n",
      "mistune              0.8.4               \n",
      "mne                  0.18.2              \n",
      "nbconvert            5.6.0               \n",
      "nbformat             4.4.0               \n",
      "notebook             6.0.1               \n",
      "numba                0.45.1              \n",
      "numpy                1.17.0              \n",
      "opt-einsum           3.0.1               \n",
      "pandas               0.24.2              \n",
      "pandocfilters        1.4.2               \n",
      "parso                0.5.1               \n",
      "pexpect              4.7.0               \n",
      "pickleshare          0.7.5               \n",
      "Pillow               6.1.0               \n",
      "pip                  19.2.2              \n",
      "pkg-resources        0.0.0               \n",
      "plotly               4.1.0               \n",
      "prometheus-client    0.7.1               \n",
      "prompt-toolkit       2.0.9               \n",
      "protobuf             3.9.1               \n",
      "ptyprocess           0.6.0               \n",
      "pycparser            2.19                \n",
      "Pygments             2.4.2               \n",
      "pyparsing            2.4.2               \n",
      "pyrsistent           0.15.4              \n",
      "python-dateutil      2.8.0               \n",
      "pytz                 2019.2              \n",
      "PyYAML               5.1.2               \n",
      "pyzmq                18.1.0              \n",
      "qtconsole            4.5.4               \n",
      "requests             2.22.0              \n",
      "resampy              0.2.2               \n",
      "retrying             1.3.3               \n",
      "scikit-learn         0.21.0              \n",
      "scipy                1.3.1               \n",
      "Send2Trash           1.5.0               \n",
      "setuptools           41.2.0              \n",
      "six                  1.12.0              \n",
      "SoundFile            0.10.2              \n",
      "tb-nightly           1.15.0a20190822     \n",
      "termcolor            1.1.0               \n",
      "terminado            0.8.2               \n",
      "testpath             0.4.2               \n",
      "tf-estimator-nightly 1.14.0.dev2019082101\n",
      "tf-nightly-gpu       1.15.0.dev20190730  \n",
      "torch                0.4.0               \n",
      "torchvision          0.2.0               \n",
      "tornado              6.0.3               \n",
      "tqdm                 4.35.0              \n",
      "traitlets            4.3.2               \n",
      "urllib3              1.25.3              \n",
      "visdom               0.1.8.8             \n",
      "wcwidth              0.1.7               \n",
      "webcolors            1.10                \n",
      "webencodings         0.5.1               \n",
      "websocket-client     0.56.0              \n",
      "Werkzeug             0.15.5              \n",
      "wheel                0.33.6              \n",
      "widgetsnbextension   3.5.1               \n",
      "wrapt                1.11.2              \n",
      "\u001b[33mWARNING: You are using pip version 19.2.2, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "# import librosa as lb\n",
    "import sys\n",
    "# import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "from keras.utils import plot_model,to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from iter_window import window \n",
    "# import speechpy as sp\n",
    "# import statistics\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/media/hinton/F8E62A5EE62A1D7E/rohit/spoof/spoof_deep_features/DS_10283_853/protocol/CM_protocol/cm_train.trn\"\n",
    "\n",
    "# open the file for reading\n",
    "filehandle = open(filename, 'r')\n",
    "train_protocol = []\n",
    "while True:\n",
    "    # read a single line\n",
    "    line = (filehandle.readline())\n",
    "    train_protocol.append(line)\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "# close the pointer to that file\n",
    "filehandle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_dev = \"/media/grace/Drive1/rohit/spoof/spoof_deep_features/DS_10283_853/protocol/CM_protocol/cm_develop.ndx\"\n",
    "\n",
    "# open the file for reading\n",
    "filehandle_dev = open(filename_dev, 'r')\n",
    "dev_protocol = []\n",
    "while True:\n",
    "    # read a single line\n",
    "    line = (filehandle_dev.readline())\n",
    "    dev_protocol.append(line)\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "# close the pointer to that file\n",
    "filehandle_dev.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_protocol1 = []\n",
    "for i in train_protocol:\n",
    "    i = i.replace('\\n','')\n",
    "    train_protocol1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_protocol1 = []\n",
    "for i in dev_protocol:\n",
    "    i = i.replace('\\n','')\n",
    "    dev_protocol1.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_protocol = dev_protocol1\n",
    "print(dev_protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_protocol = train_protocol1\n",
    "print(train_protocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = []\n",
    "train_label = []\n",
    "for i in train_protocol:\n",
    "    j = i.split(' ')[1:2]\n",
    "    train_pd.append(j)\n",
    "    train_label.append(i.split(' ')[3:4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd1 = []\n",
    "for i in train_pd:\n",
    "    train_pd1.append(str(i)[2:-2])\n",
    "train_pd = train_pd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pd = []\n",
    "dev_label = []\n",
    "for i in dev_protocol:\n",
    "    dev_pd.append(i.split(' ')[1:2])\n",
    "    dev_label.append(i.split(' ')[3:4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_pd1 = []\n",
    "for i in dev_pd:\n",
    "    dev_pd1.append(str(i)[2:-2])\n",
    "dev_pd = dev_pd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import names of files in dataset\n",
    "path = r'/media/hinton/F8E62A5EE62A1D7E/rohit/spoof/spoof_deep_features/DS_10283_853/wav/'\n",
    "files = []\n",
    "missing=[]\n",
    "print(path)\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.wav' in file  :        \n",
    "            files.append(os.path.join(r, file))\n",
    "        else:\n",
    "            missing.append(file)\n",
    "print(len(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get train and dev set\n",
    "file_name=[]\n",
    "for i in missing:\n",
    "    i = i.split('/')[-1].replace('.wav','')\n",
    "    file_name.append(i)\n",
    "\n",
    "file_set = set()\n",
    "train_pd_set = set()\n",
    "# dev_pd_set = set()\n",
    "file_set = set(file_name)\n",
    "train_pd_set = set(train_pd)\n",
    "# dev_pd_set = set(dev_pd)\n",
    "\n",
    "train_set = file_set & train_pd_set\n",
    "# dev_set = file_set & dev_pd_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code below ensures that train samples and train_labels are in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = []\n",
    "label_set = []\n",
    "for k in train_set:\n",
    "    for i in train_protocol1:\n",
    "        j = str(i.split(' ')[1:2])[2:-2]\n",
    "        if j==k:\n",
    "            label_set.append(j)\n",
    "            train_label.append(i.split(' ')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_label = []\n",
    "for i in dev_protocol:\n",
    "    j = str(i.split(' ')[1:2])[2:-2]\n",
    "    if j in dev_set:\n",
    "        dev_label.append(i.split(' ')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"train_label.npy\",train_label)\n",
    "# np.save(\"dev_label.npy\",dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save audio data of trainset\n",
    "train_audio= []\n",
    "for count, i in enumerate(train_set):\n",
    "#     print(i)\n",
    "    j = i.split('_')[0]\n",
    "    train_audio.append(lb.load('/media/grace/Drive1/rohit/spoof/spoof_deep_features/DS_10283_853/wav/'+j+'/'+i+'.wav'))\n",
    "    if count%1000==0 and count==1000:\n",
    "        print('saving')\n",
    "        np.save(\"train\"+str(count)+\".npy\",train_audio[0:count])\n",
    "    elif count%1000==0 and count >= 1000:\n",
    "        print('saving')\n",
    "        np.save(\"train\"+str(count)+\".npy\",train_audio[count-1000:count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save audio data of trainset\n",
    "from os import path\n",
    "train_audio= []\n",
    "missing_audio = []\n",
    "for count, i in enumerate(train_set):\n",
    "#     print(i)\n",
    "    j = i.split('_')[0]\n",
    "    if path.exists('/media/hinton/F8E62A5EE62A1D7E/rohit/spoof/spoof_deep_features/DS_10283_853/wav/'+j+'/'+i+'.wav'):\n",
    "        train_audio.append(i)\n",
    "    else:\n",
    "        missing_audio.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_set_list = list(train_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_protocol1[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save .npy file of audio data of devset\n",
    "dev_audio= []\n",
    "for count,i in enumerate(dev_set):\n",
    "#     print(i)\n",
    "    j = i.split('_')[0]\n",
    "    dev_audio.append(lb.load('/media/grace/Drive1/rohit/spoof/spoof_deep_features/DS_10283_853/wav/'+j+'/'+i+'.wav'))\n",
    "    if count%1000==0 and count==1000:\n",
    "        print('saving')\n",
    "        np.save(\"dev\"+str(count)+\".npy\",dev_audio[0:count])\n",
    "    elif count%1000==0 and count >= 1000:\n",
    "        print('saving')\n",
    "        np.save(\"dev\"+str(count)+\".npy\",dev_audio[count-1000:count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "path = r'/media/grace/Drive1/rohit/spoof/spoof_deep_features/weights'\n",
    "print(path)\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.npy' in file and 'train' in file and '_' not in file:        \n",
    "            train.append(np.load(os.path.join(r, file), allow_pickle = True))\n",
    "# print(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = []\n",
    "path = r'/media/grace/Drive1/rohit/spoof/spoof_deep_features/'\n",
    "print(path)\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.npy' in file and 'dev' in file and '_' not in file:        \n",
    "            dev.append(np.load(os.path.join(r, file), allow_pickle = True))\n",
    "print(len(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train[16][1]\n",
    "train_format = []\n",
    "for i in range(len(train)):\n",
    "    for j in train[i]:\n",
    "        train_format.append(j[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_format = []\n",
    "for i in range(len(dev)):\n",
    "    for j in dev[i]:\n",
    "        dev_format.append(j[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_format= np.array(train_format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_format = np.array(dev_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb.feature.mfcc(f,22050, n_mfcc=60).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta = []\n",
    "delta2 = []\n",
    "#use librosa to get mfcc features\n",
    "mfcc_feat_train = np.empty((16000,17880))\n",
    "\n",
    "for count,f in enumerate(train_format):\n",
    "    delta=np.array(lb.feature.delta(lb.feature.mfcc(f,22050, n_mfcc=60)))\n",
    "    delta2=np.array(lb.feature.delta(lb.feature.mfcc(f,22050, n_mfcc=60), order=2))\n",
    "    value =pad_sequences(sp.processing.cmvnw(np.concatenate([delta,delta2], axis=None).reshape(1,-1)),maxlen=17880,dtype='float32')\n",
    "    mfcc_feat_train[count] = value\n",
    "    if count%100==0:\n",
    "        print(count)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = np.array([[5, 6]])\n",
    "np.concatenate((a, b), axis=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feat_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_train = []\n",
    "# for i in mfcc_feat_train:\n",
    "#     window_train.append(list((window(i[0],7,7,padding=0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_length =[]\n",
    "for i in mfcc_feat_train:\n",
    "    mfcc_length.append(i.shape[1])\n",
    "print(max(mfcc_length))  \n",
    "len(mfcc_feat_train)\n",
    "median = statistics.median(mfcc_length)\n",
    "print(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find median of the length of windowed features to padd or chop\n",
    "# length_windows = []\n",
    "# for i in range(len(window_train)):\n",
    "#     length_windows.append(len(window_train[i]))\n",
    "    \n",
    "# median = statistics.median(length_windows)\n",
    "\n",
    "\n",
    "# pad_window = pad_sequences(window_train,maxlen=17880,dtype='float32')\n",
    "\n",
    "# pad_window[1][17879]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.empty((16000,17880),dtype = 'float32')\n",
    "# for x,i in enumerate(mfcc_feat):\n",
    "#     for y,j in enumerate(i):\n",
    "#         out = pad_sequences(j.reshape(1,-1),maxlen=17880,dtype='float32')\n",
    "#         data[x][y] = out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mfcc_trian_padded.npy\",mfcc_feat_train,allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta = []\n",
    "# delta2 = []\n",
    "# #use librosa to get mfcc features\n",
    "# mfcc_feat_dev= []\n",
    "# for f in dev_format:\n",
    "# #     mfcc_feat_train.append(np.array((lb.features.delta(lb.feature.mfcc(f[0],22050))).flatten()))\n",
    "#     delta=np.array(lb.feature.delta(lb.feature.mfcc(f,22050, n_mfcc=60)))\n",
    "#     delta2=np.array(lb.feature.delta(lb.feature.mfcc(f,22050, n_mfcc=60), order=2))\n",
    "# #     mfcc_feat_train = np.append(np.array(mfcc_feat_train),np.concatenate([delta,delta2], axis = 1))\n",
    "#    mfcc_feat_dev.append(np.concatenate([delta,delta2], axis = 1))\n",
    "\n",
    "# mfcc_length = []\n",
    "# for i in mfcc_feat_dev:\n",
    "#     mfcc_length.append(i.shape[1])\n",
    "# print(max(mfcc_length))    \n",
    "\n",
    "# data = np.empty((len(mfcc_feat_dev),60,max(mfcc_length)),dtype = 'float32')\n",
    "# for x,i in enumerate(mfcc_feat_dev):\n",
    "#     for y,j in enumerate(i):\n",
    "#         out = pad_sequences(j.reshape(1,-1),maxlen=max(mfcc_length),dtype='float32')\n",
    "#         data[x][y] = out\n",
    "\n",
    "\n",
    "# np.save(\"mffc_dev\",data,allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dev = np.load(\"mffc_dev.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load(\"mfcc_trian_padded.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = data_train.reshape(16000,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(\"train_label.npy\")\n",
    "\n",
    "train_labels=list(train_labels)\n",
    "\n",
    "train_labels1 = list()\n",
    "for i in train_labels:\n",
    "    if i == b'human':\n",
    "        train_labels1.append(0)\n",
    "    else:\n",
    "        train_labels1.append(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels1.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scalers = {}\n",
    "# x_train,y_train = np.load(\"train_data1.npy\"), np.load(\"train_labels1.npy\")\n",
    "# for i in range(x_train.shape[1]):\n",
    "#     scalers[i] = MinMaxScaler()\n",
    "#     x_train[:, i, :] = scalers[i].fit_transform(x_train[:, i, :]) \n",
    "# x_validation,y_validation = np.load(\"validation_data1.npy\"), np.load(\"validation_labels1.npy\")\n",
    "# for i in range(x_validation.shape[1]):\n",
    "#     x_validation[:, i, :] = scalers[i].transform(x_validation[:, i, :]) \n",
    "# x_test,y_test = np.load(\"test_data1.npy\"), np.load(\"test_labels1.npy\")\n",
    "# for i in range(x_test.shape[1]):\n",
    "#     x_test[:, i, :] = scalers[i].transform(x_test[:, i, :]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define neural network\n",
    "output = []\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1000, activation='sigmoid', input_shape = (17880,)))\n",
    "model.add(Dense(1000, activation='sigmoid'))\n",
    "model.add(Dense(1000, activation='sigmoid'))\n",
    "model.add(Dense(1000, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='linear'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# tb = TensorBoard(log_dir=\"/media/grace/Drive1/rohit/spoof/spoof_deep_features/visuals/logs/{}\".format(time()))\n",
    "\n",
    "# filepath=\"/media/grace/Drive1/rohit/spoof/spoof_deep_features/weights/weights.best.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# callbacks_list = [checkpoint]\n",
    "\n",
    "# model.load_weights(\"/media/grace/Drive1/rohit/spoof/spoof_deep_features/weights/weights.best.hdf5\")\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "layeroutputs = []\n",
    "# for i in pad_window:\n",
    "model.fit(data_train, to_categorical(train_labels1[0:16000]), epochs = 10,batch_size = 64)#, callbacks = callbacks_list, validation_data=(x_validation,y_validation))\n",
    "get_5th_layer_output = K.function([model.layers[0].input],\n",
    "                                 [model.layers[4].output])\n",
    "output = get_5th_layer_output([data_train])[0]\n",
    "    \n",
    "\n",
    "# y_pred =  model.predict(x_test)\n",
    "# y_pred = np.round(y_pred[:,1])\n",
    "# print(metrics.accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"BNF.npy\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Labels\n",
    "## 0 : Human and 1 : spoof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embedding=np.load('BNF.npy')\n",
    "train_labels = np.load(\"train_label.npy\")\n",
    "\n",
    "train_labels=list(train_labels)\n",
    "\n",
    "train_labels1 = list()\n",
    "for i in train_labels:\n",
    "    if i == b'human':\n",
    "        train_labels1.append(0)\n",
    "    else:\n",
    "        train_labels1.append(1)\n",
    "train_labels = train_labels1[0:16000]\n",
    "\n",
    "# rus = RandomUnderSampler(random_state=0)\n",
    "# X_resampled, y_resampled = rus.fit_resample(data_embedding, train_labels1[:16000])\n",
    "# X,Y = shuffle(X_resampled,y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(train_labels, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seperate the zeros and the ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j , k = 0,0\n",
    "# human_samples = np.empty((3674,17880)) # wrong array size as BNF is of size 3674,64\n",
    "human_samples = np.empty((3674,64))# corrected\n",
    "human_labels= []\n",
    "# spoof_samples = np.empty((12326,17880))\n",
    "spoof_samples = np.empty((12326,64))\n",
    "spoof_labels = []\n",
    "for count,i in enumerate(train_labels):\n",
    "    if i ==0:\n",
    "#         human_samples=np.append(human_samples,data_embedding[count])# Do not append insert the data at that position\n",
    "        human_samples[j]=data_embedding[count]\n",
    "        human_labels.append(train_labels[count])\n",
    "        j = j+1\n",
    "    elif i ==1:\n",
    "#         spoof_samples=np.append(spoof_samples,data_embedding[count])\n",
    "        spoof_samples[k] = data_embedding[count]\n",
    "        spoof_labels.append(train_labels[count])\n",
    "        k = k+1\n",
    "        \n",
    "print(len(human_labels))\n",
    "print(len(spoof_labels))\n",
    "print((human_samples[3]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"human_samples.npy\",human_samples,allow_pickle=True)\n",
    "np.save(\"spoof_samples.npy\",spoof_samples,allow_pickle=True)\n",
    "np.save(\"human_labels.npy\",human_labels)\n",
    "np.save(\"spoof_labels.npy\",spoof_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM for human samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_samples = np.load(\"human_samples.npy\")\n",
    "human_labels = np.load(\"human_labels.npy\")\n",
    "spoof_samples = np.load(\"spoof_samples.npy\")\n",
    "spoof_labels = np.load(\"spoof_labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM(n_components=512).fit(human_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = gmm.predict(human_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(human_samples[:, 0], human_samples[:, 1], c=y_predicted, s=40, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_predicted, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(human_labels, y_predicted, pos_label=1)\n",
    "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "thresh = interp1d(fpr, threshold)(eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit GMM for spoof samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_spoof = GMM(n_components=512).fit(spoof_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_spoof = gmm.predict(spoof_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_predicted_spoof, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(spoof_labels, y_predicted_spoof, pos_label=1)\n",
    "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "thresh = interp1d(fpr, threshold)(eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit GMM for all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embedding=np.load('BNF.npy')\n",
    "train_labels = np.load(\"train_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=list(train_labels)\n",
    "\n",
    "train_labels1 = list()\n",
    "for i in train_labels:\n",
    "    if i == b'human':\n",
    "        train_labels1.append(0)\n",
    "    else:\n",
    "        train_labels1.append(1)\n",
    "train_labels = train_labels1[0:16000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GMM(n_components=512,covariance_type='full', random_state=0).fit(data_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = gmm.predict(data_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_predicted, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(train_labels, y_predicted, pos_label=1)\n",
    "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "thresh = interp1d(fpr, threshold)(eer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
