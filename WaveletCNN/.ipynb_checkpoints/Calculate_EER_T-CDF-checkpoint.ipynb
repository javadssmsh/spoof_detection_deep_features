{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "import numpy as np\n",
    "# import librosa as lb\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import AveragePooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical,Sequence\n",
    "import pandas as pd\n",
    "import math\n",
    "import sincnet\n",
    "# from tensorflow import set_random_seed\n",
    "from keras import models, layers\n",
    "import numpy as np\n",
    "import sincnet\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import MaxPooling1D,MaxPooling2D, Conv1D, LeakyReLU, BatchNormalization, Dense, Flatten\n",
    "from keras.layers import InputLayer, Input\n",
    "from keras.models import Model\n",
    "# from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pywt\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import tensorflow.contrib.slim as slim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Yields the next training batch.\n",
    "    Suppose `samples` is an array [[audio1,label1], [audio2,label2],...].\n",
    "    \"\"\"\n",
    "    total_batches = int(data.shape[0]/batch_size)\n",
    "    for i in range(total_batches):\n",
    "        X_train,y_train = create_batches_rnd(data, labels, batch_size,i)\n",
    "        yield X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches_rnd_val(data,labels,batch_size,batch_number):\n",
    "    wlen = 3200\n",
    "    fact_amp = 0.2\n",
    "    # Initialization of the minibatch (batch_size,[0=>x_t,1=>x_t+N,1=>random_samp])\n",
    "    sig_batch=np.zeros([batch_size,wlen])\n",
    "    lab_batch=[]\n",
    "    if batch_number == 0:\n",
    "        signal_id_arr=np.array([s for s in range(0,128)])\n",
    "    else:\n",
    "        offset = (batch_number*128) \n",
    "        signal_id_arr=np.array([s for s in range(offset,offset+128)])\n",
    "    rand_amp_arr = np.random.uniform(1.0-fact_amp,1+fact_amp,batch_size)\n",
    "    for i in range(batch_size): \n",
    "        signal = data[signal_id_arr[i]]\n",
    "        # accesing to a random chunk\n",
    "        signal_len=signal.shape[0]\n",
    "        signal_beg=np.random.randint(signal_len-wlen-1) #randint(0, snt_len-2*wlen-1)\n",
    "        signal_end=signal_beg+wlen\n",
    "        sig_batch[i,:]=signal[signal_beg:signal_end]*rand_amp_arr[i]\n",
    "        y=labels.iloc[signal_id_arr[i],-1]\n",
    "        lab_batch.append(y)\n",
    "    a, b = np.shape(sig_batch)\n",
    "    sig_batch = sig_batch.reshape((a, b, 1))\n",
    "    return sig_batch, to_categorical(np.array(lab_batch),num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_conv_block(X,in_channels,out_channels,stage,block,dilation=1):\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = BatchNormalization(name=bn_name_base+'a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv1D(in_channels, 3, padding='valid',use_bias = False, name= conv_name_base+'a')(X)\n",
    "    X = BatchNormalization(name=bn_name_base+'b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv1D(in_channels, 3, padding='valid',use_bias = False, name= conv_name_base+'b')(X)\n",
    "    print(X.shape)\n",
    "    paddings = tf.constant([[0, 0],   # the batch size dimension\n",
    "                          [2, 2],   # top and bottom of image\n",
    "                          [0, 0]])  # the channels dimension\n",
    "    X = Lambda(lambda x: tf.pad(x, paddings, mode='CONSTANT',\n",
    "                        constant_values=0.0))(X)\n",
    "    X = concatenate([X , X_shortcut])\n",
    "    X = BatchNormalization(name = bn_name_base+'c')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv1D(out_channels, 3, padding='valid',use_bias = False, dilation_rate = dilation, name = conv_name_base+'c')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    input_shape = None,3200,1\n",
    "        \n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32, shape=input_shape, name= 'the_input')\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    sinc = sincnet.SincConv1D(64, 251, 16000)(inputs)\n",
    "    sinc_pool = MaxPooling1D(pool_size=3,name = 'sinc_pool')(sinc)\n",
    "    sinc_norm = BatchNormalization(momentum=0.05, name = 'sinc_norm')(sinc_pool)\n",
    "    sinc_layer_norm = sincnet.LayerNorm(name = 'sinc_layer_norm')(sinc_norm)\n",
    "    sinc_relu = LeakyReLU(alpha=0.2, name = 'sinc_relu')(sinc_layer_norm)\n",
    "    \n",
    "    sinc_conv = Conv1D(64, 5, strides=2, padding='valid',kernel_initializer = keras.initializers.glorot_uniform(seed=0))(sinc_relu)\n",
    "    # sinc_pool_1 = MaxPooling1D(pool_size=3,name = 'sinc_pool_1')(sinc_conv)\n",
    "    sinc_norm_1 = BatchNormalization(momentum=0.05, name = 'sinc_norm_1')(sinc_conv)\n",
    "    sinc_layer_norm_1 = sincnet.LayerNorm(name = 'sinc_layer_norm_1')(sinc_norm_1)\n",
    "    sinc_relu_1 = LeakyReLU(alpha=0.2, name = 'sinc_relu_1')(sinc_layer_norm_1)\n",
    "     \n",
    "    #concate level one and level two decomposition\n",
    "    # concate_level_2 = concatenate([relu_1_2,sinc_relu_1])\n",
    "    # print(concate_level_2.shape)\n",
    "    res_conv_1 = res_conv_block(sinc_relu_1, 128, 16, 1, 'a', 4)\n",
    "    res_conv_2 = res_conv_block(res_conv_1, 16, 8, 2, 'a', 8)\n",
    "    res_conv_3 = res_conv_block(res_conv_2, 8, 4, 3, 'a', 16)\n",
    "    res_conv_4 = res_conv_block(res_conv_3, 4, 2, 4, 'a', 32)\n",
    "    res_conv_5 = res_conv_block(res_conv_4, 2, 1, 5, 'a', 64)\n",
    "    \n",
    "    res_norm = BatchNormalization(name='res_norm')(res_conv_5)\n",
    "    res_relu = Activation('relu')(res_norm)\n",
    "    \n",
    "    \n",
    "    pool_5_1 = AveragePooling1D(pool_size=3, padding='same', name='avg_pool_5_1')(res_relu)\n",
    "    flat_5_1 = Flatten(name='flat_5_1')(pool_5_1) \n",
    "    \n",
    "    fc_5 = Dense(2048, name='fc_5',kernel_initializer = keras.initializers.glorot_uniform(seed=0))(flat_5_1)\n",
    "    norm_5 = BatchNormalization(name='norm_5')(fc_5)\n",
    "    relu_5 = Activation('relu', name='relu_5')(norm_5)\n",
    "    drop_5 = Dropout(0.5, name='drop_5')(relu_5)\n",
    "    \n",
    "    fc_6 = Dense(2048, name='fc_6',kernel_initializer = keras.initializers.glorot_uniform(seed=0))(drop_5)\n",
    "    norm_6 = BatchNormalization(name='norm_6')(fc_6)\n",
    "    relu_6 = Activation('relu', name='relu_6')(norm_6)\n",
    "    drop_6 = Dropout(0.5, name='drop_6')(relu_6)\n",
    "    \n",
    "    output = Dense(2, activation=tf.nn.softmax)(drop_6)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 486, 128)\n",
      "(?, 478, 16)\n",
      "(?, 462, 8)\n",
      "(?, 430, 4)\n",
      "(?, 366, 2)\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/home/rohita/rohit/spoof/npy_data_asvspoof/Sincnet/equal_human_spoof'\n",
    "saver = tf.train.Saver()\n",
    "# save_path = saver.save(sess, model_path)\n",
    "with sess.as_default():\n",
    "    latest = tf.train.latest_checkpoint(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rohita/rohit/spoof/npy_data_asvspoof/Sincnet/equal_human_spoof/model.ckpt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
