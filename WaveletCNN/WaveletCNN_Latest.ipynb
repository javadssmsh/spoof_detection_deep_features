{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "VnEyfmhu8mN6",
    "outputId": "50232f8b-d446-4e4b-cabe-fff7d1b32ca2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RnfSUDv183Cz",
    "outputId": "7fd3e060-112c-45d3-d65f-583f89e41dcf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/SA/Code/spoof_detection_deep_features/WaveletCNN\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/SA/Code/spoof_detection_deep_features/WaveletCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Collecting keras==2.0.8\n",
      "  Using cached Keras-2.0.8-py2.py3-none-any.whl (276 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from keras==2.0.8) (1.11.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from keras==2.0.8) (1.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras==2.0.8) (3.12)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from keras==2.0.8) (1.15.1)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.2.4\n",
      "    Uninstalling Keras-2.2.4:\n",
      "      Successfully uninstalled Keras-2.2.4\n",
      "Successfully installed keras-2.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 79
    },
    "colab_type": "code",
    "id": "J3R7P0a08iUH",
    "outputId": "e76cefff-ee7e-409b-fb13-7cc01f3857f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "from keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zQyX8ddJk32y"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "import numpy as np\n",
    "import librosa as lb\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import AveragePooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical,Sequence\n",
    "import pandas as pd\n",
    "import math\n",
    "import sincnet\n",
    "# from tensorflow import set_random_seed\n",
    "from keras import models, layers\n",
    "import numpy as np\n",
    "import sincnet\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import MaxPooling1D,MaxPooling2D, Conv1D, LeakyReLU, BatchNormalization, Dense, Flatten\n",
    "from keras.layers import InputLayer, Input\n",
    "from keras.models import Model\n",
    "# from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pywt\n",
    "from sklearn.decomposition import PCA\n",
    "import sys\n",
    "import tensorflow.contrib.slim as slim\n",
    "# import tfwavelets\n",
    "# sys.path.insert(1, '/content/drive/My Drive/SA/Code/spoof_detection_deep_features/WaveletCNN/cwt-tensorflow')\n",
    "# from cwt import cwtMortlet, cwtRicker,mortletWavelet, rickerWavelet\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "\n",
    "# import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p85kAJ4i9B8a"
   },
   "outputs": [],
   "source": [
    "img = tf.placeholder(tf.float32, shape=(None, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "08soF5zXGSGy"
   },
   "outputs": [],
   "source": [
    "# def framing_windowing(signal):\n",
    "#   pre_emphasis = 0.97\n",
    "#   frame_size = 256\n",
    "#   frame_stride = 128\n",
    "#   nfilt = 20\n",
    "#   NFFT = 511\n",
    "#   emphasized_signal = tf.concat([tf.reshape(signal[:,0],[tf.shape(signal)[0],1]),(signal[:,1:] - pre_emphasis * signal[:,:-1])],axis =1)\n",
    "#   print(emphasized_signal.shape)\n",
    "#   frame_length, frame_step = frame_size, frame_stride  # Convert from seconds to samples\n",
    "#   signal_frames=tf.signal.frame(emphasized_signal,frame_length, frame_step)\n",
    "#   signal_frames *=tf.signal.hamming_window(frame_length)\n",
    "#   print(signal_frames.shape)\n",
    "#   return signal_frames\n",
    "def framing_windowing(signal):\n",
    "    pre_emphasis = 0.97\n",
    "    frame_size = 3200\n",
    "    frame_stride = 160\n",
    "    nfilt = 20\n",
    "    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "    frame_length, frame_step = frame_size, frame_stride  # Convert from seconds to samples\n",
    "    signal_length = len(emphasized_signal)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n",
    "#     print(num_frames)\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    frames *= np.hamming(frame_length)\n",
    "    # print(frames.shape)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jwHzqvPGu3bi"
   },
   "outputs": [],
   "source": [
    "# def tkeo_new(a):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Calculates the TKEO of a given recording by using 2 samples.\n",
    "#     See Li et al., 2007\n",
    "#     Arguments:\n",
    "#     a \t\t\t--- 1D numpy array.\n",
    "#     Returns:\n",
    "#     1D numpy array containing the tkeo per sample\n",
    "#     \"\"\"\n",
    "#     # Create two temporary arrays of equal length, shifted 1 sample to the right\n",
    "#     # and left and squared:\n",
    "#     i = a[:,:,1:-1]*a[:,:,1:-1]\n",
    "#     j = a[:,:,2:]*a[:,:,:-2]\n",
    "#     # Calculate the difference between the two temporary arrays:\n",
    "#     aTkeo = i-j\n",
    "#     return aTkeo\n",
    "def tkeo(a):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the TKEO of a given recording by using 2 samples.\n",
    "    See Li et al., 2007\n",
    "    Arguments:\n",
    "    a \t\t\t--- 1D numpy array.\n",
    "    Returns:\n",
    "    1D numpy array containing the tkeo per sample\n",
    "    \"\"\"\n",
    "    # Create two temporary arrays of equal length, shifted 1 sample to the right\n",
    "    # and left and squared:\n",
    "    i = a[1:-1]*a[1:-1]\n",
    "    j = a[2:]*a[:-2]\n",
    "    # Calculate the difference between the two temporary arrays:\n",
    "    aTkeo = i-j\n",
    "    return aTkeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(data, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Yields the next training batch.\n",
    "    Suppose `samples` is an array [[audio1,label1], [audio2,label2],...].\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        X_train,y_train = create_batches_rnd(data, labels, batch_size)\n",
    "        yield X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_val(data, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Yields the next training batch.\n",
    "    Suppose `samples` is an array [[audio1,label1], [audio2,label2],...].\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        X_train,y_train = create_batches_rnd(data, labels, batch_size)\n",
    "        yield X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_part1(data, labels,batch_size):\n",
    "    \"\"\"\n",
    "    Yields the next training batch.\n",
    "    Suppose `samples` is an array [[audio1,label1], [audio2,label2],...].\n",
    "    \"\"\"\n",
    "    x_samples = []\n",
    "    y_samples = []\n",
    "    for count,i in enumerate(data):\n",
    "        x_samples = framing_windowing(i)\n",
    "        num_samples = len(x_samples)\n",
    "        y_samples = [labels[count]]*num_samples\n",
    "#         print(\"yo\")\n",
    " \n",
    "      # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "        # Get the samples you'll use in this batch\n",
    "            batch_samples = x_samples[offset:offset+batch_size]\n",
    "\n",
    "        # Initialise X_train and y_train arrays for this batch\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            wpt = []\n",
    "        # For each example\n",
    "            for count,x_sample in enumerate(batch_samples):\n",
    "            # audio (X) and label (y)\n",
    "                audio =  x_sample\n",
    "#             print(level1.shape)\n",
    "                label = y_samples[count]\n",
    "            \n",
    "            # Add example to arrays\n",
    "                X_train.append(audio)\n",
    "                y_train.append(label)\n",
    "                wpt.append(Wavelet_1d(audio))\n",
    "\n",
    "\n",
    "        # Make sure they're numpy arrays (as opposed to lists)\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "#             print(y_train)\n",
    "#             y_train = to_categorical(y_train)\n",
    "            wpt = np.array(wpt)\n",
    "#             print(y_train.shape)\n",
    "        # The generator-y part: yield the next training batch            \n",
    "            yield X_train,wpt, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches_rnd(data,labels,batch_size):\n",
    "    wlen = 3200\n",
    "    fact_amp = 0.2\n",
    "    # Initialization of the minibatch (batch_size,[0=>x_t,1=>x_t+N,1=>random_samp])\n",
    "    sig_batch=np.zeros([batch_size,wlen])\n",
    "    wpt_batch = np.zeros([batch_size,wlen,1])\n",
    "    lab_batch=[]\n",
    "    signal_id_arr=np.random.randint(data.shape[0], size=batch_size)\n",
    "    rand_amp_arr = np.random.uniform(1.0-fact_amp,1+fact_amp,batch_size)\n",
    "    for i in range(batch_size): \n",
    "        # select a random sentence from the list \n",
    "        #[fs,signal]=scipy.io.wavfile.read(data_folder+wav_lst[snt_id_arr[i]])\n",
    "        #signal=signal.astype(float)/32768\n",
    "#         [signal, fs] = sf.read(data_folder+wav_lst[snt_id_arr[i]])\n",
    "        signal = data[signal_id_arr[i]]\n",
    "        # accesing to a random chunk\n",
    "        signal_len=signal.shape[0]\n",
    "        signal_beg=np.random.randint(signal_len-wlen-1) #randint(0, snt_len-2*wlen-1)\n",
    "        signal_end=signal_beg+wlen\n",
    "        sig_batch[i,:]=signal[signal_beg:signal_end]*rand_amp_arr[i]\n",
    "        wpt = Wavelet_1d(signal[signal_beg:signal_end])\n",
    "        wpt_batch[i,:] = wpt\n",
    "        y=labels[signal_id_arr[i]]\n",
    "#         yt = to_categorical(y, num_classes=out_dim)\n",
    "        lab_batch.append(y)\n",
    "    a, b = np.shape(sig_batch)\n",
    "    sig_batch = sig_batch.reshape((a, b, 1))\n",
    "    return sig_batch, wpt_batch, np.array(lab_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches_rnd(data,labels,batch_size):\n",
    "    wlen = 3200\n",
    "    fact_amp = 0.2\n",
    "    # Initialization of the minibatch (batch_size,[0=>x_t,1=>x_t+N,1=>random_samp])\n",
    "    sig_batch=np.zeros([batch_size,wlen])\n",
    "#     wpt_batch = np.zeros([batch_size,wlen,1])\n",
    "    lab_batch=[]\n",
    "    signal_id_arr=np.random.randint(data.shape[0], size=batch_size)\n",
    "    rand_amp_arr = np.random.uniform(1.0-fact_amp,1+fact_amp,batch_size)\n",
    "    for i in range(batch_size): \n",
    "        # select a random sentence from the list \n",
    "        #[fs,signal]=scipy.io.wavfile.read(data_folder+wav_lst[snt_id_arr[i]])\n",
    "        #signal=signal.astype(float)/32768\n",
    "#         [signal, fs] = sf.read(data_folder+wav_lst[snt_id_arr[i]])\n",
    "        signal = data[signal_id_arr[i]]\n",
    "        # accesing to a random chunk\n",
    "        signal_len=signal.shape[0]\n",
    "        signal_beg=np.random.randint(signal_len-wlen-1) #randint(0, snt_len-2*wlen-1)\n",
    "        signal_end=signal_beg+wlen\n",
    "        sig_batch[i,:]=signal[signal_beg:signal_end]*rand_amp_arr[i]\n",
    "#         wpt = Wavelet_1d(signal[signal_beg:signal_end])\n",
    "#         wpt_batch[i,:] = wpt\n",
    "        y=labels[signal_id_arr[i]]\n",
    "#         yt = to_categorical(y, num_classes=out_dim)\n",
    "        lab_batch.append(y)\n",
    "    a, b = np.shape(sig_batch)\n",
    "    sig_batch = sig_batch.reshape((a, b, 1))\n",
    "    return sig_batch, np.array(lab_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_val(x_samples, y_samples, batch_size):\n",
    "    \"\"\"\n",
    "    Yields the next training batch.\n",
    "    Suppose `samples` is an array [[audio1,label1], [audio2,label2],...].\n",
    "    \"\"\"\n",
    "    \n",
    "    num_samples = len(x_samples)\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        \n",
    "        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            # Get the samples you'll use in this batch\n",
    "            batch_samples = x_samples[offset:offset+batch_size]\n",
    "\n",
    "            # Initialise X_train and y_train arrays for this batch\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            wpt = []\n",
    "            # For each example\n",
    "            for count,x_sample in enumerate(batch_samples):\n",
    "                # audio (X) and label (y)\n",
    "                audio =  x_sample\n",
    "#                 print(level1.shape)\n",
    "                label = y_samples[count]\n",
    "                \n",
    "                # Add example to arrays\n",
    "                X_train.append(audio)\n",
    "                y_train.append(label)\n",
    "                wpt.append(Wavelet_1d(audio))\n",
    "\n",
    "\n",
    "            # Make sure they're numpy arrays (as opposed to lists)\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            y_train = to_categorical(y_train)\n",
    "            wpt = np.array(wpt)\n",
    "#             print(\"hello\")\n",
    "            # The generator-y part: yield the next training batch            \n",
    "            yield X_train, wpt, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_val_part1(data, labels):\n",
    "    \"\"\"\n",
    "    Yields the next training batch.\n",
    "    Suppose `samples` is an array [[audio1,label1], [audio2,label2],...].\n",
    "    \"\"\"\n",
    "    x_samples = []\n",
    "    y_samples = []\n",
    "    for count,i in enumerate(data):\n",
    "        x_samples = framing_windowing(i)\n",
    "        num_samples = len(x_samples)\n",
    "        y_samples = [labels[count]]*num_samples\n",
    "        # Get index to start each batch: [0, batch_size, 2*batch_size, ..., max multiple of batch_size <= num_samples]\n",
    "        \n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            # Get the samples you'll use in this batch\n",
    "            batch_samples = x_samples[offset:offset+batch_size]\n",
    "\n",
    "            # Initialise X_train and y_train arrays for this batch\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            wpt = []\n",
    "            # For each example\n",
    "            for count,x_sample in enumerate(batch_samples):\n",
    "                # audio (X) and label (y)\n",
    "                audio =  x_sample\n",
    "#                 print(level1.shape)\n",
    "                label = y_samples[count]\n",
    "                \n",
    "                # Add example to arrays\n",
    "                X_train.append(audio)\n",
    "                y_train.append(label)\n",
    "                wpt.append(Wavelet_1d(audio))\n",
    "\n",
    "\n",
    "            # Make sure they're numpy arrays (as opposed to lists)\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(y_train)\n",
    "            wpt = np.array(wpt)\n",
    "#             print(\"hello\")\n",
    "            # The generator-y part: yield the next training batch            \n",
    "            yield X_train, wpt, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16ezLo4M6WQM"
   },
   "outputs": [],
   "source": [
    "# def Wavelet_1d(batch_signal):\n",
    "#   import numpy\n",
    "#   pre_emphasis = 0.97\n",
    "#   frame_size = 256\n",
    "#   frame_stride = 128\n",
    "#   nfilt = 20\n",
    "#   NFFT = 511\n",
    "#   sample_rate = 16000\n",
    "#   batch_signal = framing_windowing(batch_signal)\n",
    "#   batch_signal = tkeo_new(batch_signal)\n",
    "#   wptree = pywt.WaveletPacket(data=batch_signal.eval(session = sess), wavelet='db1', mode='symmetric')\n",
    "#   level1 = wptree.get_level(1, order = \"freq\")\n",
    "#   level2 = wptree.get_level(2, order = \"freq\")\n",
    "#   level3 = wptree.get_level(3, order = \"freq\")\n",
    "#   level4 = wptree.get_level(4, order = \"freq\")\n",
    "#   # print(\"level1 data array:\",np.array(level1).shape)\n",
    "#   # print(\"level2 data array:\",np.array(level2).shape)\n",
    "#   # print(\"level3 data array:\",np.array(level3).shape)\n",
    "#   # print(\"level4 data array:\",np.array(level4).shape)\n",
    "#   for node in level1:\n",
    "#     data_wp = node.data\n",
    "#     # print(\"WP data:\",np.array(data_wp).shape)\n",
    "#     # Features group\n",
    "#     frame_features1.extend(data_wp)\n",
    "#   mag_frames = numpy.absolute(frame_features1)  # Magnitude of the FFT\n",
    "#   pow_frames = numpy.abs((mag_frames) ** 2)\n",
    "#   z = mel.shape[1] - pow_frames.shape[0]\n",
    "#   pow_frames = np.pad(pow_frames,[(0,z)],'constant', constant_values=0)\n",
    "#   # print(\"pow_frames\",pow_frames.shape)\n",
    "#   mel_scaled_features = mel.dot(pow_frames)\n",
    "#   audio_features1.append(mel_scaled_features)\n",
    "# #   for count,signal in enumerate(signals):\n",
    "# #     audio_features1 = []\n",
    "# #     audio_features2 = []\n",
    "# #     audio_features3 = []\n",
    "# #     audio_features4 = []\n",
    "# #     for f in signal:\n",
    "# #       tke = tkeo(f)\n",
    "# #       data_std = StandardScaler().fit_transform(tke.reshape(-1,1)).reshape(1,-1)[0]            \n",
    "              \n",
    "# #       #Feature extraction for each node\n",
    "# #       frame_features1 = []\n",
    "# #       frame_features2 = []\n",
    "# #       frame_features3 = []\n",
    "# #       frame_features4 = []        \n",
    "      \n",
    "\n",
    "# #       for node in level2:\n",
    "# #         data_wp = node.data\n",
    "# #         # print(\"WP data:\",np.array(data_wp).shape)\n",
    "# #         # Features group\n",
    "# #         frame_features2.extend(data_wp)\n",
    "# #       mag_frames = numpy.absolute(frame_features2)  # Magnitude of the FFT\n",
    "# #       pow_frames = numpy.abs((mag_frames) ** 2)\n",
    "# #       mel_scaled_features = mel.dot(pow_frames)\n",
    "# #       audio_features2.append(mel_scaled_features)\n",
    "# #       for node in level3:\n",
    "# #         data_wp = node.data\n",
    "# #         # print(\"WP data:\",np.array(data_wp).shape)\n",
    "# #         # Features group\n",
    "# #         frame_features3.extend(data_wp)\n",
    "# #       mag_frames = numpy.absolute(frame_features3)  # Magnitude of the FFT\n",
    "# #       pow_frames = numpy.abs((mag_frames) ** 2)\n",
    "# #       mel_scaled_features = mel.dot(pow_frames)\n",
    "# #       audio_features3.append(mel_scaled_features)\n",
    "# #       for node in level4:\n",
    "# #         data_wp = node.data\n",
    "# #         # print(\"WP data:\",np.array(data_wp).shape)\n",
    "# #         # Features group\n",
    "# #         frame_features4.extend(data_wp)\n",
    "# #       mag_frames = numpy.absolute(frame_features4)  # Magnitude of the FFT\n",
    "# #       pow_frames = numpy.abs((mag_frames) ** 2)\n",
    "# #       mel_scaled_features = mel.dot(pow_frames)\n",
    "# #       audio_features4.append(mel_scaled_features)\n",
    "    \n",
    "# #     # print(\"audio_features1:\",np.array(audio_features1).shape)\n",
    "# #     # print(\"audio_features1:\",np.array(audio_features2).shape)\n",
    "# #     # print(\"audio_features1:\",np.array(audio_features3).shape)\n",
    "# #     # print(\"audio_features1:\",np.array(audio_features4).shape)\n",
    "# # #     print(\"hello\")\n",
    "# #     log_energy1 = numpy.log10(audio_features1)\n",
    "# #     log_energy1 = pd.DataFrame(log_energy1)\n",
    "# #     log_energy2 = numpy.log10(audio_features2)\n",
    "# #     log_energy2 = pd.DataFrame(log_energy2)\n",
    "# #     log_energy3 = numpy.log10(audio_features3)\n",
    "# #     log_energy3 = pd.DataFrame(log_energy3)\n",
    "# #     log_energy4 = numpy.log10(audio_features4)\n",
    "# #     log_energy4 = pd.DataFrame(log_energy4)\n",
    "# #     pd.set_option('use_inf_as_null', True)\n",
    "# #     log_energy1=log_energy1.fillna(log_energy1.mean())\n",
    "# #     log_energy2=log_energy2.fillna(log_energy2.mean())\n",
    "# #     log_energy3=log_energy3.fillna(log_energy3.mean())\n",
    "# #     log_energy4=log_energy4.fillna(log_energy4.mean())\n",
    "# #     # print(\"log_energy1: \",log_energy1.shape)\n",
    "# #     # print(\"log_energy2: \",log_energy2.shape)\n",
    "# #     # print(\"log_energy3: \",log_energy3.shape)\n",
    "# #     # print(\"log_energy4: \",log_energy4.shape)\n",
    "# #     signals_level1[count] = np.array(log_energy1)\n",
    "# #     signals_level2[count] = np.array(log_energy2)\n",
    "# #     signals_level3[count] = np.array(log_energy3)\n",
    "# #     signals_level4[count] = np.array(log_energy4)\n",
    "# #   # print(signals_level1.shape)\n",
    "# #   # print(signals_level2.shape)\n",
    "# #   # print(signals_level3.shape)\n",
    "# #   # print(signals_level4.shape)  \n",
    "\n",
    "  return [signals_level1,signals_level2,signals_level3,signals_level4]\n",
    "\n",
    "def Wavelet_out_shape(input_shapes):\n",
    "    # print('in to shape')\n",
    "    return [tuple([None, 590, 20]), tuple([None, 590, 20]), \n",
    "            tuple([None, 590, 20]), tuple([None, 590, 20])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wavelet_1d(signal):\n",
    "    import numpy\n",
    "    nfilt = 3200\n",
    "    NFFT = 8191\n",
    "    sample_rate = 16000\n",
    "    # signal = tkeo(signal)\n",
    "    mel = lb.filters.mel(sr=sample_rate, n_fft=NFFT, n_mels=nfilt)\n",
    "    audio_features = []\n",
    "    tke = tkeo(signal)\n",
    "    data_std = StandardScaler().fit_transform(tke.reshape(-1,1)).reshape(1,-1)[0]            \n",
    "    wptree = pywt.WaveletPacket(data=data_std, wavelet='db1', mode='symmetric')\n",
    "    level = wptree.maxlevel\n",
    "    levels = wptree.get_level(level, order = \"freq\")            \n",
    "        #Feature extraction for each node\n",
    "    frame_features = []        \n",
    "    for node in levels:\n",
    "        data_wp = node.data\n",
    "            # Features group\n",
    "        frame_features.extend(data_wp)\n",
    "#     print(np.array(frame_features).shape)\n",
    "    mag_frames = numpy.absolute(frame_features)  # Magnitude of the FFT\n",
    "    pow_frames = numpy.abs((mag_frames) ** 2)\n",
    "    mel_scaled_features = mel.dot(pow_frames)\n",
    "    log_energy = numpy.log10(mel_scaled_features)\n",
    "    log_energy = pd.DataFrame(log_energy)\n",
    "    pd.set_option('use_inf_as_null', True)\n",
    "    log_energy=log_energy.fillna(log_energy.mean())\n",
    "    return np.array(log_energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rTbkidLybP7E"
   },
   "outputs": [],
   "source": [
    "def Wavelet_1d(signal):\n",
    "    import numpy\n",
    "    pre_emphasis = 0.97\n",
    "    frame_size = 256\n",
    "    frame_stride = 128\n",
    "    nfilt = 20\n",
    "    NFFT = 511\n",
    "    sample_rate = 16000\n",
    "    signal = framing_windowing(signal)\n",
    "    # signal = tkeo(signal)\n",
    "    mel = lb.filters.mel(sr=sample_rate, n_fft=NFFT, n_mels=nfilt)\n",
    "    audio_features = []\n",
    "    for f in signal:\n",
    "        tke = tkeo(f)\n",
    "        data_std = StandardScaler().fit_transform(tke.reshape(-1,1)).reshape(1,-1)[0]            \n",
    "        wptree = pywt.WaveletPacket(data=data_std, wavelet='db1', mode='symmetric')\n",
    "        level = wptree.maxlevel\n",
    "        levels = wptree.get_level(level, order = \"freq\")            \n",
    "        #Feature extraction for each node\n",
    "        frame_features = []        \n",
    "        for node in levels:\n",
    "            data_wp = node.data\n",
    "            # Features group\n",
    "            frame_features.extend(data_wp)\n",
    "#         print(len(frame_features))\n",
    "        mag_frames = numpy.absolute(frame_features)  # Magnitude of the FFT\n",
    "        pow_frames = numpy.abs((mag_frames) ** 2)\n",
    "        mel_scaled_features = mel.dot(pow_frames)\n",
    "        audio_features.append(mel_scaled_features)\n",
    "    \n",
    "    \n",
    "#     print(\"hello\")\n",
    "    log_energy = numpy.log10(audio_features)\n",
    "    log_energy = pd.DataFrame(log_energy)\n",
    "    pd.set_option('use_inf_as_null', True)\n",
    "    log_energy=log_energy.fillna(log_energy.mean())\n",
    "    return np.array(log_energy)\n",
    "def Wavelet_out_shape(input_shapes):\n",
    "    # print('in to shape')\n",
    "    return [tuple([None, 590, 20]), tuple([None, 590, 20]), \n",
    "            tuple([None, 590, 20]), tuple([None, 590, 20])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(399, 20)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wavelet_1d(X_train[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0pWATBebNAW"
   },
   "outputs": [],
   "source": [
    "def Wavelet_1d(signal):\n",
    "    import numpy\n",
    "    pre_emphasis = 0.97\n",
    "    frame_size = 256\n",
    "    frame_stride = 128\n",
    "    nfilt = 20\n",
    "    NFFT = 511\n",
    "    sample_rate = 16000\n",
    "    signal = framing_windowing(signal)\n",
    "    # signal = tkeo(signal)\n",
    "    mel = lb.filters.mel(sr=sample_rate, n_fft=NFFT, n_mels=nfilt)\n",
    "    audio_features1 = []\n",
    "    audio_features2 = []\n",
    "    audio_features3 = []\n",
    "    audio_features4 = []\n",
    "    # print(\"signal shape\",signal.shape)\n",
    "    for f in signal:\n",
    "        tke = tkeo(f)\n",
    "        data_std = StandardScaler().fit_transform(tke.reshape(-1,1)).reshape(1,-1)[0]            \n",
    "        wptree = pywt.WaveletPacket(data=data_std, wavelet='db1', mode='symmetric')\n",
    "        level1 = wptree.get_level(1, order = \"freq\")\n",
    "        level2 = wptree.get_level(2, order = \"freq\")\n",
    "        level3 = wptree.get_level(3, order = \"freq\")\n",
    "        level4 = wptree.get_level(4, order = \"freq\")\n",
    "        # print(\"level1 data array:\",np.array(level1).shape)\n",
    "        # print(\"level2 data array:\",np.array(level2).shape)\n",
    "        # print(\"level3 data array:\",np.array(level3).shape)\n",
    "        # print(\"level4 data array:\",np.array(level4).shape)          \n",
    "          #Feature extraction for each node\n",
    "        frame_features1 = []\n",
    "        frame_features2 = []\n",
    "        frame_features3 = []\n",
    "        frame_features4 = []        \n",
    "        for node in level1:\n",
    "            data_wp = node.data\n",
    "          # print(\"WP data:\",np.array(data_wp).shape)\n",
    "          # Features group\n",
    "            frame_features1.extend(data_wp)\n",
    "        # print(\"frame_features1\",np.array(frame_features1).shape)\n",
    "        mag_frames = numpy.absolute(frame_features1)  # Magnitude of the FFT\n",
    "        pow_frames = numpy.abs((mag_frames) ** 2)\n",
    "        z = mel.shape[1] - pow_frames.shape[0]\n",
    "        pow_frames = np.pad(pow_frames,[(0,z)],'constant', constant_values=0)\n",
    "        # print(\"pow_frames\",pow_frames.shape)\n",
    "        mel_scaled_features = mel.dot(pow_frames)\n",
    "        audio_features1.append(mel_scaled_features)\n",
    "        \n",
    "    \n",
    "        for node in level2:\n",
    "            data_wp = node.data\n",
    "          # print(\"WP data:\",np.array(data_wp).shape)\n",
    "          # Features group\n",
    "            frame_features2.extend(data_wp)\n",
    "        mag_frames = numpy.absolute(frame_features2)  # Magnitude of the FFT\n",
    "        pow_frames = numpy.abs((mag_frames) ** 2)\n",
    "        mel_scaled_features = mel.dot(pow_frames)\n",
    "        audio_features2.append(mel_scaled_features)\n",
    "        for node in level3:\n",
    "            data_wp = node.data\n",
    "          # print(\"WP data:\",np.array(data_wp).shape)\n",
    "          # Features group\n",
    "            frame_features3.extend(data_wp)\n",
    "        mag_frames = numpy.absolute(frame_features3)  # Magnitude of the FFT\n",
    "        pow_frames = numpy.abs((mag_frames) ** 2)\n",
    "        mel_scaled_features = mel.dot(pow_frames)\n",
    "        audio_features3.append(mel_scaled_features)\n",
    "        for node in level4:\n",
    "            data_wp = node.data\n",
    "          # print(\"WP data:\",np.array(data_wp).shape)\n",
    "          # Features group\n",
    "            frame_features4.extend(data_wp)\n",
    "        mag_frames = numpy.absolute(frame_features4)  # Magnitude of the FFT\n",
    "        pow_frames = numpy.abs((mag_frames) ** 2)\n",
    "        mel_scaled_features = mel.dot(pow_frames)\n",
    "        audio_features4.append(mel_scaled_features)\n",
    "      \n",
    "    # print(\"audio_features1:\",np.array(audio_features1).shape)\n",
    "    # print(\"audio_features1:\",np.array(audio_features2).shape)\n",
    "    # print(\"audio_features1:\",np.array(audio_features3).shape)\n",
    "    # print(\"audio_features1:\",np.array(audio_features4).shape)\n",
    "    print(\"hello\")\n",
    "    log_energy1 = numpy.log10(audio_features1)\n",
    "    log_energy1 = pd.DataFrame(log_energy1)\n",
    "    log_energy2 = numpy.log10(audio_features2)\n",
    "    log_energy2 = pd.DataFrame(log_energy2)\n",
    "    log_energy3 = numpy.log10(audio_features3)\n",
    "    log_energy3 = pd.DataFrame(log_energy3)\n",
    "    log_energy4 = numpy.log10(audio_features4)\n",
    "    log_energy4 = pd.DataFrame(log_energy4)\n",
    "    pd.set_option('use_inf_as_na', True)\n",
    "    log_energy1=log_energy1.fillna(log_energy1.mean())\n",
    "    log_energy1 = np.array(log_energy1)\n",
    "    log_energy2=log_energy2.fillna(log_energy2.mean())\n",
    "    log_energy2 = np.array(log_energy2)\n",
    "    log_energy3=log_energy3.fillna(log_energy3.mean())\n",
    "    log_energy3 = np.array(log_energy3)\n",
    "    log_energy4=log_energy4.fillna(log_energy4.mean())\n",
    "    log_energy4 = np.array(log_energy4)\n",
    "    # print(\"log_energy1: \",log_energy1.shape)\n",
    "    # print(\"log_energy2: \",log_energy2.shape)\n",
    "    # print(\"log_energy3: \",log_energy3.shape)\n",
    "    # print(\"log_energy4: \",log_energy4.shape)\n",
    "    # signals_level1[count] = np.array(log_energy1)\n",
    "    # signals_level2[count] = np.array(log_energy2)\n",
    "    # signals_level3[count] = np.array(log_energy3)\n",
    "    # signals_level4[count] = np.array(log_energy4)\n",
    "    # print(signals_level1.shape)\n",
    "    # print(signals_level2.shape)\n",
    "    # print(signals_level3.shape)\n",
    "    # print(signals_level4.shape)  \n",
    "    \n",
    "    return log_energy1,log_energy2,log_energy3,log_energy4\n",
    "\n",
    "def Wavelet_out_shape(input_shapes):\n",
    "    # print('in to shape')\n",
    "    return [tuple([None, 590, 20]), tuple([None, 590, 20]), \n",
    "            tuple([None, 590, 20]), tuple([None, 590, 20])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "w_Mu-yIn9EMW",
    "outputId": "a435b87f-7105-457a-9ae3-0bdaf1728751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 399, 20)\n",
      "hi\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AveragePooling2D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8bc9a310040b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mrelu_5_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu_5_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_5_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mpool_5_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'avg_pool_5_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu_5_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0mflat_5_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'flat_5_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_5_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AveragePooling2D' is not defined"
     ]
    }
   ],
   "source": [
    "input_shape = None,51221\n",
    "    \n",
    "# input_ = Input(input_shape, name='the_input')\n",
    "input_ = tf.placeholder(tf.float32, shape=input_shape, name= 'the_input')\n",
    "# wavelet = Lambda(Wavelet_1d, Wavelet_out_shape, name='wavelet')\n",
    "# tke = wavelet(input_)\n",
    "input_l1 = tf.placeholder(tf.float32, shape=(None,399,20), name= 'input_l1')\n",
    "input_l2 = tf.placeholder(tf.float32, shape=(None,399,20), name= 'input_l2')\n",
    "input_l3 = tf.placeholder(tf.float32, shape=(None,399,20), name= 'input_l3')\n",
    "input_l4 = tf.placeholder(tf.float32, shape=(None,399,20), name= 'input_l4')\n",
    "input_1 = Reshape((399,20,1))(input_l1)\n",
    "input_2 = Reshape((399,20,1))(input_l2)\n",
    "input_3 = Reshape((399,20,1))(input_l3)\n",
    "input_4 = Reshape((399,20,1))(input_l4)\n",
    "print(input_l1.shape)\n",
    "# print(input_l2)\n",
    "# print(input_l3)\n",
    "# print(input_l4)\n",
    "\n",
    "\n",
    "# level one decomposition starts\n",
    "print(\"hi\")\n",
    "conv_1 = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_1')(input_1)\n",
    "# #     pool_1 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_1)\n",
    "norm_1 = BatchNormalization(name='norm_1')(conv_1)\n",
    "relu_1 = Activation('relu', name='relu_1')(norm_1)\n",
    "\n",
    "# conv_1_2 = Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_1_2')(relu_1)\n",
    "conv_1_2 = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_1_2')(relu_1)\n",
    "#     # pool_1_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_1_2)\n",
    "norm_1_2 = BatchNormalization(name='norm_1_2')(conv_1_2)\n",
    "relu_1_2 = Activation('relu', name='relu_1_2')(norm_1_2)\n",
    "    \n",
    "\n",
    "# level two decomposition starts\n",
    "conv_a = Conv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv_a')(input_2)\n",
    "#     # pool_a = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_a)\n",
    "norm_a = BatchNormalization(name='norm_a')(conv_a)\n",
    "relu_a = Activation('relu', name='relu_a')(norm_a)\n",
    "#print(relu_1_2.shape,relu_a.shape)\n",
    "    \n",
    "#concate level one and level two decomposition\n",
    "concate_level_2 = concatenate([relu_1_2, relu_a])\n",
    "conv_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_2')(concate_level_2)\n",
    "# #     pool_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_2)\n",
    "norm_2 = BatchNormalization(name='norm_2')(conv_2)\n",
    "relu_2 = Activation('relu', name='relu_2')(norm_2)\n",
    "\n",
    "# conv_2_2 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_2_2')(relu_2)\n",
    "conv_2_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_2_2')(relu_2)\n",
    "#     # pool_2_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_2_2)\n",
    "norm_2_2 = BatchNormalization(name='norm_2_2')(conv_2_2)\n",
    "relu_2_2 = Activation('relu', name='relu_2_2')(norm_2_2)\n",
    "\n",
    "#level three decomposition starts \n",
    "conv_b = Conv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv_b')(input_3)\n",
    "#     # pool_b = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_b)\n",
    "norm_b = BatchNormalization(name='norm_b')(conv_b)\n",
    "relu_b = Activation('relu', name='relu_b')(norm_b)\n",
    "\n",
    "conv_b_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_b_2')(relu_b)\n",
    "#     # pool_b_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_b_2)\n",
    "norm_b_2 = BatchNormalization(name='norm_b_2')(conv_b_2)\n",
    "relu_b_2 = Activation('relu', name='relu_b_2')(norm_b_2)\n",
    "#print(relu_2_2.shape,relu_b_2.shape)\n",
    "#concate level two and level three decomposition \n",
    "concate_level_3 = concatenate([relu_2_2, relu_b_2])\n",
    "conv_3 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_3')(concate_level_3)\n",
    "# #     pool_3 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_3)\n",
    "norm_3 = BatchNormalization(name='nomr_3')(conv_3)\n",
    "relu_3 = Activation('relu', name='relu_3')(norm_3)\n",
    "\n",
    "# conv_3_2 = Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_3_2')(relu_3)\n",
    "conv_3_2 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_3_2')(relu_3)\n",
    "#     # pool_3_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_3_2)\n",
    "norm_3_2 = BatchNormalization(name='norm_3_2')(conv_3_2)\n",
    "relu_3_2 = Activation('relu', name='relu_3_2')(norm_3_2)\n",
    "\n",
    "#level four decomposition start\n",
    "conv_c = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_c')(input_4)\n",
    "#     # pool_c = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_c)\n",
    "norm_c = BatchNormalization(name='norm_c')(conv_c)\n",
    "relu_c = Activation('relu', name='relu_c')(norm_c)\n",
    "\n",
    "conv_c_2 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_c_2')(relu_c)\n",
    "#     # pool_c_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_c_2)\n",
    "norm_c_2 = BatchNormalization(name='norm_c_2')(conv_c_2)\n",
    "relu_c_2 = Activation('relu', name='relu_c_2')(norm_c_2)\n",
    "\n",
    "conv_c_3 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_c_3')(relu_c_2)\n",
    "#     # pool_c_3 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_c_3)\n",
    "norm_c_3 = BatchNormalization(name='norm_c_3')(conv_c_3)\n",
    "relu_c_3 = Activation('relu', name='relu_c_3')(norm_c_3)\n",
    "#print(relu_3_2.shape,relu_c_3.shape)\n",
    "# concate level level three and level four decomposition\n",
    "concate_level_4 = concatenate([relu_3_2, relu_c_3])\n",
    "conv_4 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_4')(concate_level_4)\n",
    "#     # pool_4 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_4)\n",
    "norm_4 = BatchNormalization(name='norm_4')(conv_4)\n",
    "relu_4 = Activation('relu', name='relu_4')(norm_4)\n",
    "\n",
    "conv_4_2 = Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_4_2')(relu_4)\n",
    "#     # pool_4_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_4_2)\n",
    "norm_4_2 = BatchNormalization(name='norm_4_2')(conv_4_2)\n",
    "relu_4_2 = Activation('relu', name='relu_4_2')(norm_4_2)\n",
    "\n",
    "conv_5_1 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_5_1')(relu_4_2)\n",
    "#     # pool_5_1 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_5_1)\n",
    "norm_5_1 = BatchNormalization(name='norm_5_1')(conv_5_1)\n",
    "relu_5_1 = Activation('relu', name='relu_5_1')(norm_5_1)\n",
    "\n",
    "pool_5_1 = AveragePooling2D(pool_size=(3, 3), strides=1, padding='same', name='avg_pool_5_1')(relu_5_1)\n",
    "flat_5_1 = Flatten(name='flat_5_1')(pool_5_1) \n",
    "\n",
    "fc_5 = Dense(2048, name='fc_5')(flat_5_1)\n",
    "norm_5 = BatchNormalization(name='norm_5')(fc_5)\n",
    "relu_5 = Activation('relu', name='relu_5')(norm_5)\n",
    "drop_5 = Dropout(0.5, name='drop_5')(relu_5)\n",
    "\n",
    "fc_6 = Dense(2048, name='fc_6')(drop_5)\n",
    "norm_6 = BatchNormalization(name='norm_6')(fc_6)\n",
    "relu_6 = Activation('relu', name='relu_6')(norm_6)\n",
    "drop_6 = Dropout(0.5, name='drop_6')(relu_6)\n",
    "\n",
    "output = Dense(2, activation='softmax', name='fc_7')(drop_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 399, 20, 64), (None, 1860, 20)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-fb8175057cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#concate level one and level two decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mconcate_level_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrelu_1_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msinc_relu_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mconv_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcate_level_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# #     pool_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rohita/rohit/spoof/work/local/lib/python2.7/site-packages/keras/layers/merge.pyc\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0malongside\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rohita/rohit/spoof/work/local/lib/python2.7/site-packages/keras/engine/base_layer.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rohita/rohit/spoof/work/local/lib/python2.7/site-packages/keras/layers/merge.pyc\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m                              \u001b[0;34m'inputs with matching shapes '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                              \u001b[0;34m'except for the concat axis. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                              'Got inputs shapes: %s' % (input_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 399, 20, 64), (None, 1860, 20)]"
     ]
    }
   ],
   "source": [
    "input_shape = None,51221,1\n",
    "    \n",
    "# input_ = Input(input_shape, name='the_input')\n",
    "input_ = tf.placeholder(tf.float32, shape=input_shape, name= 'the_input')\n",
    "# wavelet = Lambda(Wavelet_1d, Wavelet_out_shape, name='wavelet')\n",
    "# tke = wavelet(input_)\n",
    "input_l1 = tf.placeholder(tf.float32, shape=(None,399,20), name= 'input_l1')\n",
    "input_1 = Reshape((399,20,1))(input_l1)\n",
    "\n",
    "# level one decomposition starts\n",
    "print(\"hi\")\n",
    "conv_1 = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_1')(input_1)\n",
    "# #     pool_1 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_1)\n",
    "norm_1 = BatchNormalization(name='norm_1')(conv_1)\n",
    "relu_1 = Activation('relu', name='relu_1')(norm_1)\n",
    "\n",
    "# conv_1_2 = Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_1_2')(relu_1)\n",
    "conv_1_2 = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_1_2')(relu_1)\n",
    "#     # pool_1_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_1_2)\n",
    "norm_1_2 = BatchNormalization(name='norm_1_2')(conv_1_2)\n",
    "relu_1_2 = Activation('relu', name='relu_1_2')(norm_1_2)\n",
    "    \n",
    "\n",
    "# level two decomposition starts\n",
    "sinc = sincnet.SincConv1D(20, 251, 16000)(input_)\n",
    "sinc_pool = MaxPooling1D(pool_size=3,name = 'sinc_pool')(sinc)\n",
    "sinc_norm = BatchNormalization(momentum=0.05, name = 'sinc_norm')(sinc_pool)\n",
    "sinc_layer_norm = sincnet.LayerNorm(name = 'sinc_layer_norm')(sinc_norm)\n",
    "sinc_relu = LeakyReLU(alpha=0.2, name = 'sinc_relu')(sinc_layer_norm)\n",
    "\n",
    "sinc_conv = Conv1D(20, 251, strides=3, padding='valid')(sinc_layer_norm)\n",
    "sinc_pool_1 = MaxPooling1D(pool_size=3,name = 'sinc_pool_1')(sinc_conv)\n",
    "sinc_norm_1 = BatchNormalization(momentum=0.05, name = 'sinc_norm_1')(sinc_pool_1)\n",
    "sinc_layer_norm_1 = sincnet.LayerNorm(name = 'sinc_layer_norm_1')(sinc_norm_1)\n",
    "sinc_relu_1 = LeakyReLU(alpha=0.2, name = 'sinc_relu_1')(sinc_layer_norm_1)\n",
    "\n",
    "sinc_conv_2 = Conv1D(20, 251, strides=3, padding='valid')(sinc_layer_norm)\n",
    "sinc_pool_2 = MaxPooling1D(pool_size=3,name = 'sinc_pool_1')(sinc_conv)\n",
    "sinc_norm_ = BatchNormalization(momentum=0.05, name = 'sinc_norm_1')(sinc_pool_1)\n",
    "sinc_layer_norm_1 = sincnet.LayerNorm(name = 'sinc_layer_norm_1')(sinc_norm_1)\n",
    "sinc_relu_1 = LeakyReLU(alpha=0.2, name = 'sinc_relu_1')(sinc_layer_norm_1)\n",
    "\n",
    "\n",
    "#print(relu_1_2.shape,relu_a.shape)\n",
    "    \n",
    "#concate level one and level two decomposition\n",
    "concate_level_2 = concatenate([relu_1_2, sinc_relu_1])\n",
    "conv_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_2')(concate_level_2)\n",
    "# #     pool_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_2)\n",
    "norm_2 = BatchNormalization(name='norm_2')(conv_2)\n",
    "relu_2 = Activation('relu', name='relu_2')(norm_2)\n",
    "\n",
    "# conv_2_2 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_2_2')(relu_2)\n",
    "conv_2_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_2_2')(relu_2)\n",
    "#     # pool_2_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_2_2)\n",
    "norm_2_2 = BatchNormalization(name='norm_2_2')(conv_2_2)\n",
    "relu_2_2 = Activation('relu', name='relu_2_2')(norm_2_2)\n",
    "\n",
    "#level three decomposition starts \n",
    "\n",
    "\n",
    "pool_5_1 = AveragePooling2D(pool_size=(3, 3), strides=1, padding='same', name='avg_pool_5_1')(relu_2_2)\n",
    "flat_5_1 = Flatten(name='flat_5_1')(pool_5_1) \n",
    "\n",
    "fc_5 = Dense(2048, name='fc_5')(flat_5_1)\n",
    "norm_5 = BatchNormalization(name='norm_5')(fc_5)\n",
    "relu_5 = Activation('relu', name='relu_5')(norm_5)\n",
    "drop_5 = Dropout(0.5, name='drop_5')(relu_5)\n",
    "\n",
    "fc_6 = Dense(2048, name='fc_6')(drop_5)\n",
    "norm_6 = BatchNormalization(name='norm_6')(fc_6)\n",
    "relu_6 = Activation('relu', name='relu_6')(norm_6)\n",
    "drop_6 = Dropout(0.5, name='drop_6')(relu_6)\n",
    "\n",
    "output = Dense(2, activation='softmax', name='fc_7')(drop_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "input_shape = None,3200,1\n",
    "    \n",
    "# input_ = Input(input_shape, name='the_input')\n",
    "input_ = tf.placeholder(tf.float32, shape=input_shape, name= 'the_input')\n",
    "# reshaped_input_ =  Reshape((3200,1))(input_)\n",
    "\n",
    "# wavelet = Lambda(Wavelet_1d, Wavelet_out_shape, name='wavelet')\n",
    "# tke = wavelet(input_)\n",
    "input_l1 = tf.placeholder(tf.float32, shape=(None,3200,1), name= 'input_l1')\n",
    "# reshaped_input_l1 = Reshape((399,20,1))(input_l1)\n",
    "# reshaped_input_l1 = Reshape((7980,1))(reshaped_input_l1)\n",
    "\n",
    "# level one decomposition starts\n",
    "print(\"hi\")\n",
    "# conv_1 = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_1')(input_1)\n",
    "# # #     pool_1 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_1)\n",
    "# norm_1 = BatchNormalization(name='norm_1')(conv_1)\n",
    "# relu_1 = Activation('relu', name='relu_1')(norm_1)\n",
    "\n",
    "conv_1 = Conv1D(64, 251, strides=1, padding='valid',name = 'conv_1')(input_l1)\n",
    "pool_1 = MaxPooling1D(pool_size=3,name = 'pool_1')(conv_1)\n",
    "norm_1 = BatchNormalization(momentum=0.05, name = 'norm_1')(pool_1)\n",
    "relu_1 = LeakyReLU(alpha=0.2, name = 'relu_1')(norm_1)\n",
    "\n",
    "\n",
    "# # conv_1_2 = Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_1_2')(relu_1)\n",
    "# conv_1_2 = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_1_2')(relu_1)\n",
    "# #     # pool_1_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_1_2)\n",
    "# norm_1_2 = BatchNormalization(name='norm_1_2')(conv_1_2)\n",
    "# relu_1_2 = Activation('relu', name='relu_1_2')(norm_1_2)\n",
    "\n",
    "conv_1_2 = Conv1D(64, 5, strides=2, padding='valid',name = 'conv_1_2')(relu_1)\n",
    "# pool_1_2 = MaxPooling1D(pool_size=3,name = 'pool_1')(sinc_conv)\n",
    "norm_1_2 = BatchNormalization(momentum=0.05, name = 'norm_1_2')(conv_1_2)\n",
    "relu_1_2 = LeakyReLU(alpha=0.2, name = 'relu_1_2')(norm_1_2) \n",
    "\n",
    "# paddings = tf.constant([[0, 0],   # the batch size dimension\n",
    "#                         [261, 260],   # top and bottom of image\n",
    "#                         [0, 0]])  # the channels dimension\n",
    "# padded_input = Lambda(lambda x: tf.pad(x, paddings, mode='CONSTANT',\n",
    "#                       constant_values=0.0))(relu_1_2)\n",
    "\n",
    "# level two decomposition starts\n",
    "sinc = sincnet.SincConv1D(64, 251, 16000)(input_)\n",
    "sinc_pool = MaxPooling1D(pool_size=3,name = 'sinc_pool')(sinc)\n",
    "sinc_norm = BatchNormalization(momentum=0.05, name = 'sinc_norm')(sinc_pool)\n",
    "sinc_layer_norm = sincnet.LayerNorm(name = 'sinc_layer_norm')(sinc_norm)\n",
    "sinc_relu = LeakyReLU(alpha=0.2, name = 'sinc_relu')(sinc_layer_norm)\n",
    "\n",
    "sinc_conv = Conv1D(64, 5, strides=2, padding='valid')(sinc_layer_norm)\n",
    "# sinc_pool_1 = MaxPooling1D(pool_size=3,name = 'sinc_pool_1')(sinc_conv)\n",
    "sinc_norm_1 = BatchNormalization(momentum=0.05, name = 'sinc_norm_1')(sinc_conv)\n",
    "sinc_layer_norm_1 = sincnet.LayerNorm(name = 'sinc_layer_norm_1')(sinc_norm_1)\n",
    "sinc_relu_1 = LeakyReLU(alpha=0.2, name = 'sinc_relu_1')(sinc_layer_norm_1)\n",
    "\n",
    "\n",
    "\n",
    "# sinc_conv_2 = Conv1D(20, 251, strides=1, padding='valid')(sinc_layer_norm_1)\n",
    "# sinc_pool_2 = MaxPooling1D(pool_size=3,name = 'sinc_pool_2')(sinc_conv_2)\n",
    "# sinc_norm_2 = BatchNormalization(momentum=0.05, name = 'sinc_norm_2')(sinc_pool_2)\n",
    "# sinc_layer_norm_2 = sincnet.LayerNorm(name = 'sinc_layer_norm_2')(sinc_norm_2)\n",
    "# sinc_relu_2 = LeakyReLU(alpha=0.2, name = 'sinc_relu_2')(sinc_layer_norm_2)\n",
    "\n",
    "\n",
    "#print(relu_1_2.shape,relu_a.shape)\n",
    "    \n",
    "#concate level one and level two decomposition\n",
    "concate_level_2 = concatenate([relu_1_2,sinc_relu_1])\n",
    "# conv_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_2')(concate_level_2)\n",
    "# # #     pool_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_2)\n",
    "# norm_2 = BatchNormalization(name='norm_2')(conv_2)\n",
    "# relu_2 = Activation('relu', name='relu_2')(norm_2)\n",
    "\n",
    "\n",
    "conv_2 = Conv1D(64, 5, strides=1, padding='valid',name = 'conv_2')(concate_level_2)\n",
    "# pool_1 = MaxPooling1D(pool_size=3,name = 'pool_2')(sinc_conv)\n",
    "norm_2 = BatchNormalization(momentum=0.05, name = 'norm_2')(conv_2)\n",
    "relu_2 = LeakyReLU(alpha=0.2, name = 'relu_2')(norm_2)\n",
    "\n",
    "# # conv_2_2 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_2_2')(relu_2)\n",
    "# conv_2_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_2_2')(relu_2)\n",
    "# #     # pool_2_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_2_2)\n",
    "# norm_2_2 = BatchNormalization(name='norm_2_2')(conv_2_2)\n",
    "# relu_2_2 = Activation('relu', name='relu_2_2')(norm_2_2)\n",
    "\n",
    "conv_2_2 = Conv1D(64, 5, strides=1, padding='valid',name = 'conv_2_2')(relu_2)\n",
    "# pool_2_2 = MaxPooling1D(pool_size=3,name = 'pool_1')(sinc_conv)\n",
    "norm_2_2 = BatchNormalization(momentum=0.05, name = 'norm_2_2')(conv_1_2)\n",
    "relu_2_2 = LeakyReLU(alpha=0.2, name = 'relu_2_2')(norm_1_2) \n",
    "\n",
    "#level three decomposition starts \n",
    "\n",
    "\n",
    "pool_5_1 = AveragePooling1D(pool_size=3, padding='same', name='avg_pool_5_1')(relu_2_2)\n",
    "flat_5_1 = Flatten(name='flat_5_1')(pool_5_1) \n",
    "\n",
    "fc_5 = Dense(2048, name='fc_5')(flat_5_1)\n",
    "norm_5 = BatchNormalization(name='norm_5')(fc_5)\n",
    "relu_5 = Activation('relu', name='relu_5')(norm_5)\n",
    "drop_5 = Dropout(0.5, name='drop_5')(relu_5)\n",
    "\n",
    "fc_6 = Dense(2048, name='fc_6')(drop_5)\n",
    "norm_6 = BatchNormalization(name='norm_6')(fc_6)\n",
    "relu_6 = Activation('relu', name='relu_6')(norm_6)\n",
    "drop_6 = Dropout(0.5, name='drop_6')(relu_6)\n",
    "\n",
    "output = Dense(2, activation=tf.nn.softmax)(drop_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_conv_block(X,in_channels,out_channels,stage,block,dilation=1):\n",
    "\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    X_shortcut = X\n",
    "    \n",
    "    X = BatchNormalization(name=bn_name_base+'a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv1D(in_channels, 3, padding='valid',use_bias = False, name= conv_name_base+'a')(X)\n",
    "    X = BatchNormalization(name=bn_name_base+'b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv1D(in_channels, 3, padding='valid',use_bias = False, name= conv_name_base+'b')(X)\n",
    "    print(X.shape)\n",
    "    paddings = tf.constant([[0, 0],   # the batch size dimension\n",
    "                          [2, 2],   # top and bottom of image\n",
    "                          [0, 0]])  # the channels dimension\n",
    "    X = Lambda(lambda x: tf.pad(x, paddings, mode='CONSTANT',\n",
    "                        constant_values=0.0))(X)\n",
    "    X = concatenate([X , X_shortcut])\n",
    "    X = BatchNormalization(name = bn_name_base+'c')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = Conv1D(out_channels, 3, padding='valid',use_bias = False, dilation_rate = dilation, name = conv_name_base+'c')(X)\n",
    "\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 486, 128)\n",
      "(?, 478, 16)\n",
      "(?, 462, 8)\n",
      "(?, 430, 4)\n",
      "(?, 366, 2)\n"
     ]
    }
   ],
   "source": [
    "input_shape = None,3200,1\n",
    "    \n",
    "\n",
    "input_ = tf.placeholder(tf.float32, shape=input_shape, name= 'the_input')\n",
    "\n",
    "# input_l1 = tf.placeholder(tf.float32, shape=(None,3200,1), name= 'input_l1')\n",
    "\n",
    "# # level one decomposition starts\n",
    "# print(\"hi\")\n",
    "# conv_1 = Conv1D(64, 251, strides=1, padding='valid',name = 'conv_1',kernel_initializer = keras.initializers.glorot_uniform(seed=0))(input_l1)\n",
    "# pool_1 = MaxPooling1D(pool_size=3,name = 'pool_1')(conv_1)\n",
    "# norm_1 = BatchNormalization(momentum=0.05, name = 'norm_1')(pool_1)\n",
    "# relu_1 = LeakyReLU(alpha=0.2, name = 'relu_1')(norm_1)\n",
    "\n",
    "\n",
    "# conv_1_2 = Conv1D(64, 5, strides=2, padding='valid',name = 'conv_1_2',kernel_initializer = keras.initializers.glorot_uniform(seed=0))(relu_1)\n",
    "# # pool_1_2 = MaxPooling1D(pool_size=3,name = 'pool_1')(sinc_conv)\n",
    "# norm_1_2 = BatchNormalization(momentum=0.05, name = 'norm_1_2')(conv_1_2)\n",
    "# relu_1_2 = LeakyReLU(alpha=0.2, name = 'relu_1_2')(norm_1_2) \n",
    "\n",
    "\n",
    "\n",
    "# level two decomposition starts\n",
    "sinc = sincnet.SincConv1D(64, 251, 16000)(input_)\n",
    "sinc_pool = MaxPooling1D(pool_size=3,name = 'sinc_pool')(sinc)\n",
    "sinc_norm = BatchNormalization(momentum=0.05, name = 'sinc_norm')(sinc_pool)\n",
    "sinc_layer_norm = sincnet.LayerNorm(name = 'sinc_layer_norm')(sinc_norm)\n",
    "sinc_relu = LeakyReLU(alpha=0.2, name = 'sinc_relu')(sinc_layer_norm)\n",
    "\n",
    "sinc_conv = Conv1D(64, 5, strides=2, padding='valid',kernel_initializer = keras.initializers.glorot_uniform(seed=0))(sinc_layer_norm)\n",
    "# sinc_pool_1 = MaxPooling1D(pool_size=3,name = 'sinc_pool_1')(sinc_conv)\n",
    "sinc_norm_1 = BatchNormalization(momentum=0.05, name = 'sinc_norm_1')(sinc_conv)\n",
    "sinc_layer_norm_1 = sincnet.LayerNorm(name = 'sinc_layer_norm_1')(sinc_norm_1)\n",
    "sinc_relu_1 = LeakyReLU(alpha=0.2, name = 'sinc_relu_1')(sinc_layer_norm_1)\n",
    " \n",
    "#concate level one and level two decomposition\n",
    "# concate_level_2 = concatenate([relu_1_2,sinc_relu_1])\n",
    "# print(concate_level_2.shape)\n",
    "res_conv_1 = res_conv_block(sinc_relu_1, 128, 16, 1, 'a', 4)\n",
    "res_conv_2 = res_conv_block(res_conv_1, 16, 8, 2, 'a', 8)\n",
    "res_conv_3 = res_conv_block(res_conv_2, 8, 4, 3, 'a', 16)\n",
    "res_conv_4 = res_conv_block(res_conv_3, 4, 2, 4, 'a', 32)\n",
    "res_conv_5 = res_conv_block(res_conv_4, 2, 1, 5, 'a', 64)\n",
    "\n",
    "res_norm = BatchNormalization(name='res_norm')(res_conv_5)\n",
    "res_relu = Activation('relu')(res_norm)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#level three decomposition starts \n",
    "\n",
    "\n",
    "pool_5_1 = AveragePooling1D(pool_size=3, padding='same', name='avg_pool_5_1')(res_relu)\n",
    "flat_5_1 = Flatten(name='flat_5_1')(pool_5_1) \n",
    "\n",
    "fc_5 = Dense(2048, name='fc_5',kernel_initializer = keras.initializers.glorot_uniform(seed=0))(flat_5_1)\n",
    "norm_5 = BatchNormalization(name='norm_5')(fc_5)\n",
    "relu_5 = Activation('relu', name='relu_5')(norm_5)\n",
    "drop_5 = Dropout(0.5, name='drop_5')(relu_5)\n",
    "\n",
    "fc_6 = Dense(2048, name='fc_6',kernel_initializer = keras.initializers.glorot_uniform(seed=0))(drop_5)\n",
    "norm_6 = BatchNormalization(name='norm_6')(fc_6)\n",
    "relu_6 = Activation('relu', name='relu_6')(norm_6)\n",
    "drop_6 = Dropout(0.5, name='drop_6')(relu_6)\n",
    "\n",
    "output = Dense(2, activation=tf.nn.softmax)(drop_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. A future version of pip will drop support for Python 2.7. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Collecting keras==2.2.4\n",
      "  Using cached Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (1.0.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/rohita/rohit/spoof/work/lib/python2.7/site-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (1.11.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (2.7.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (3.12)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/rohita/rohit/spoof/work/lib/python2.7/site-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from keras==2.2.4) (1.15.1)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.1.3\n",
      "    Uninstalling Keras-2.1.3:\n",
      "      Successfully uninstalled Keras-2.1.3\n",
      "Successfully installed keras-2.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    print(shape)\n",
    "    print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "Variables: name (type shape) [size]\n",
      "---------\n",
      "sinc_conv1d_1/filt_b1:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_conv1d_1/filt_band:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_conv1d_1/Variable:0 (float32_ref 251) [251, bytes: 1004]\n",
      "sinc_conv1d_1/Variable_1:0 (float32_ref 125) [125, bytes: 500]\n",
      "sinc_conv1d_1/Variable_2:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_3:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_4:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_5:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_6:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_7:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_8:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_9:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_10:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_11:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_12:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_13:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_14:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_15:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_16:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_17:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_18:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_19:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_20:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_21:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_22:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_23:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_24:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_25:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_26:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_27:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_28:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_29:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_30:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_31:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_32:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_33:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_34:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_35:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_36:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_37:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_38:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_39:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_40:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_41:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_42:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_43:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_44:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_45:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_46:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_47:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_48:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_49:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_50:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_51:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_52:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_53:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_54:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_55:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_56:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_57:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_58:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_59:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_60:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_61:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_62:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_63:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_64:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_65:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_66:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_67:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_68:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_69:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_70:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_71:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_72:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_73:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_74:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_75:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_76:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_77:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_78:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_79:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_80:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_81:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_82:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_83:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_84:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_85:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_86:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_87:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_88:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_89:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_90:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_91:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_92:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_93:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_94:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_95:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_96:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_97:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_98:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_99:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_100:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_101:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_102:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_103:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_104:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_105:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_106:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_107:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_108:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_109:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_110:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_111:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_112:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_113:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_114:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_115:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_116:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_117:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_118:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_119:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_120:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_121:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_122:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_123:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_124:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_125:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_126:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_127:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_128:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_129:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_130:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_131:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_132:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_133:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_134:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_135:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_136:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_137:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_138:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_139:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_140:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_141:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_142:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_143:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_144:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_145:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_146:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_147:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_148:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_149:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_150:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_151:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_152:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_153:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_154:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_155:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_156:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_157:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_158:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_159:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_160:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_161:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_162:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_163:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_164:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_165:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_166:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_167:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_168:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_169:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_170:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_171:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_172:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_173:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_174:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_175:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_176:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_177:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_178:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_179:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_180:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_181:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_182:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_183:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_184:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_185:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_186:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_187:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_188:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_189:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_190:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_191:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_192:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_193:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_194:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_195:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_196:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_197:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_198:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_199:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_200:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_201:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_202:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_203:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_204:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_205:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_206:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_207:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_208:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_209:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_210:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_211:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_212:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_213:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_214:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_215:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_216:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_217:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_218:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_219:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_220:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_221:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_222:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_223:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_224:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_225:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_226:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_227:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_228:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_229:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_230:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_231:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_232:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_233:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_234:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_235:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_236:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_237:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_238:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_239:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_240:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_241:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_242:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_243:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_244:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_245:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_246:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_247:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_248:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_249:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_250:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_251:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_252:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_253:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_254:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_255:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_256:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_conv1d_1/Variable_257:0 (float32_ref 1) [1, bytes: 4]\n",
      "sinc_norm/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_norm/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_norm/moving_mean:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_norm/moving_variance:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_layer_norm/sinc_layer_norm_scale:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_layer_norm/sinc_layer_norm_bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "conv1d_1/kernel:0 (float32_ref 5x64x64) [20480, bytes: 81920]\n",
      "conv1d_1/bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_norm_1/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_norm_1/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_norm_1/moving_mean:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_norm_1/moving_variance:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_layer_norm_1/sinc_layer_norm_1_scale:0 (float32_ref 64) [64, bytes: 256]\n",
      "sinc_layer_norm_1/sinc_layer_norm_1_bias:0 (float32_ref 64) [64, bytes: 256]\n",
      "bn1a_brancha/gamma:0 (float32_ref 64) [64, bytes: 256]\n",
      "bn1a_brancha/beta:0 (float32_ref 64) [64, bytes: 256]\n",
      "bn1a_brancha/moving_mean:0 (float32_ref 64) [64, bytes: 256]\n",
      "bn1a_brancha/moving_variance:0 (float32_ref 64) [64, bytes: 256]\n",
      "res1a_brancha/kernel:0 (float32_ref 3x64x128) [24576, bytes: 98304]\n",
      "bn1a_branchb/gamma:0 (float32_ref 128) [128, bytes: 512]\n",
      "bn1a_branchb/beta:0 (float32_ref 128) [128, bytes: 512]\n",
      "bn1a_branchb/moving_mean:0 (float32_ref 128) [128, bytes: 512]\n",
      "bn1a_branchb/moving_variance:0 (float32_ref 128) [128, bytes: 512]\n",
      "res1a_branchb/kernel:0 (float32_ref 3x128x128) [49152, bytes: 196608]\n",
      "bn1a_branchc/gamma:0 (float32_ref 192) [192, bytes: 768]\n",
      "bn1a_branchc/beta:0 (float32_ref 192) [192, bytes: 768]\n",
      "bn1a_branchc/moving_mean:0 (float32_ref 192) [192, bytes: 768]\n",
      "bn1a_branchc/moving_variance:0 (float32_ref 192) [192, bytes: 768]\n",
      "res1a_branchc/kernel:0 (float32_ref 3x192x16) [9216, bytes: 36864]\n",
      "bn2a_brancha/gamma:0 (float32_ref 16) [16, bytes: 64]\n",
      "bn2a_brancha/beta:0 (float32_ref 16) [16, bytes: 64]\n",
      "bn2a_brancha/moving_mean:0 (float32_ref 16) [16, bytes: 64]\n",
      "bn2a_brancha/moving_variance:0 (float32_ref 16) [16, bytes: 64]\n",
      "res2a_brancha/kernel:0 (float32_ref 3x16x16) [768, bytes: 3072]\n",
      "bn2a_branchb/gamma:0 (float32_ref 16) [16, bytes: 64]\n",
      "bn2a_branchb/beta:0 (float32_ref 16) [16, bytes: 64]\n",
      "bn2a_branchb/moving_mean:0 (float32_ref 16) [16, bytes: 64]\n",
      "bn2a_branchb/moving_variance:0 (float32_ref 16) [16, bytes: 64]\n",
      "res2a_branchb/kernel:0 (float32_ref 3x16x16) [768, bytes: 3072]\n",
      "bn2a_branchc/gamma:0 (float32_ref 32) [32, bytes: 128]\n",
      "bn2a_branchc/beta:0 (float32_ref 32) [32, bytes: 128]\n",
      "bn2a_branchc/moving_mean:0 (float32_ref 32) [32, bytes: 128]\n",
      "bn2a_branchc/moving_variance:0 (float32_ref 32) [32, bytes: 128]\n",
      "res2a_branchc/kernel:0 (float32_ref 3x32x8) [768, bytes: 3072]\n",
      "bn3a_brancha/gamma:0 (float32_ref 8) [8, bytes: 32]\n",
      "bn3a_brancha/beta:0 (float32_ref 8) [8, bytes: 32]\n",
      "bn3a_brancha/moving_mean:0 (float32_ref 8) [8, bytes: 32]\n",
      "bn3a_brancha/moving_variance:0 (float32_ref 8) [8, bytes: 32]\n",
      "res3a_brancha/kernel:0 (float32_ref 3x8x8) [192, bytes: 768]\n",
      "bn3a_branchb/gamma:0 (float32_ref 8) [8, bytes: 32]\n",
      "bn3a_branchb/beta:0 (float32_ref 8) [8, bytes: 32]\n",
      "bn3a_branchb/moving_mean:0 (float32_ref 8) [8, bytes: 32]\n",
      "bn3a_branchb/moving_variance:0 (float32_ref 8) [8, bytes: 32]\n",
      "res3a_branchb/kernel:0 (float32_ref 3x8x8) [192, bytes: 768]\n",
      "bn3a_branchc/gamma:0 (float32_ref 16) [16, bytes: 64]\n",
      "bn3a_branchc/beta:0 (float32_ref 16) [16, bytes: 64]\n",
      "bn3a_branchc/moving_mean:0 (float32_ref 16) [16, bytes: 64]\n",
      "bn3a_branchc/moving_variance:0 (float32_ref 16) [16, bytes: 64]\n",
      "res3a_branchc/kernel:0 (float32_ref 3x16x4) [192, bytes: 768]\n",
      "bn4a_brancha/gamma:0 (float32_ref 4) [4, bytes: 16]\n",
      "bn4a_brancha/beta:0 (float32_ref 4) [4, bytes: 16]\n",
      "bn4a_brancha/moving_mean:0 (float32_ref 4) [4, bytes: 16]\n",
      "bn4a_brancha/moving_variance:0 (float32_ref 4) [4, bytes: 16]\n",
      "res4a_brancha/kernel:0 (float32_ref 3x4x4) [48, bytes: 192]\n",
      "bn4a_branchb/gamma:0 (float32_ref 4) [4, bytes: 16]\n",
      "bn4a_branchb/beta:0 (float32_ref 4) [4, bytes: 16]\n",
      "bn4a_branchb/moving_mean:0 (float32_ref 4) [4, bytes: 16]\n",
      "bn4a_branchb/moving_variance:0 (float32_ref 4) [4, bytes: 16]\n",
      "res4a_branchb/kernel:0 (float32_ref 3x4x4) [48, bytes: 192]\n",
      "bn4a_branchc/gamma:0 (float32_ref 8) [8, bytes: 32]\n",
      "bn4a_branchc/beta:0 (float32_ref 8) [8, bytes: 32]\n",
      "bn4a_branchc/moving_mean:0 (float32_ref 8) [8, bytes: 32]\n",
      "bn4a_branchc/moving_variance:0 (float32_ref 8) [8, bytes: 32]\n",
      "res4a_branchc/kernel:0 (float32_ref 3x8x2) [48, bytes: 192]\n",
      "bn5a_brancha/gamma:0 (float32_ref 2) [2, bytes: 8]\n",
      "bn5a_brancha/beta:0 (float32_ref 2) [2, bytes: 8]\n",
      "bn5a_brancha/moving_mean:0 (float32_ref 2) [2, bytes: 8]\n",
      "bn5a_brancha/moving_variance:0 (float32_ref 2) [2, bytes: 8]\n",
      "res5a_brancha/kernel:0 (float32_ref 3x2x2) [12, bytes: 48]\n",
      "bn5a_branchb/gamma:0 (float32_ref 2) [2, bytes: 8]\n",
      "bn5a_branchb/beta:0 (float32_ref 2) [2, bytes: 8]\n",
      "bn5a_branchb/moving_mean:0 (float32_ref 2) [2, bytes: 8]\n",
      "bn5a_branchb/moving_variance:0 (float32_ref 2) [2, bytes: 8]\n",
      "res5a_branchb/kernel:0 (float32_ref 3x2x2) [12, bytes: 48]\n",
      "bn5a_branchc/gamma:0 (float32_ref 4) [4, bytes: 16]\n",
      "bn5a_branchc/beta:0 (float32_ref 4) [4, bytes: 16]\n",
      "bn5a_branchc/moving_mean:0 (float32_ref 4) [4, bytes: 16]\n",
      "bn5a_branchc/moving_variance:0 (float32_ref 4) [4, bytes: 16]\n",
      "res5a_branchc/kernel:0 (float32_ref 3x4x1) [12, bytes: 48]\n",
      "res_norm/gamma:0 (float32_ref 1) [1, bytes: 4]\n",
      "res_norm/beta:0 (float32_ref 1) [1, bytes: 4]\n",
      "res_norm/moving_mean:0 (float32_ref 1) [1, bytes: 4]\n",
      "res_norm/moving_variance:0 (float32_ref 1) [1, bytes: 4]\n",
      "fc_5/kernel:0 (float32_ref 81x2048) [165888, bytes: 663552]\n",
      "fc_5/bias:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "norm_5/gamma:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "norm_5/beta:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "norm_5/moving_mean:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "norm_5/moving_variance:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "fc_6/kernel:0 (float32_ref 2048x2048) [4194304, bytes: 16777216]\n",
      "fc_6/bias:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "norm_6/gamma:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "norm_6/beta:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "norm_6/moving_mean:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "norm_6/moving_variance:0 (float32_ref 2048) [2048, bytes: 8192]\n",
      "dense_1/kernel:0 (float32_ref 2048x2) [4096, bytes: 16384]\n",
      "dense_1/bias:0 (float32_ref 2) [2, bytes: 8]\n",
      "Total size of variables: 4494866\n",
      "Total bytes of variables: 17979464\n"
     ]
    }
   ],
   "source": [
    "def model_summary():\n",
    "    model_vars = tf.trainable_variables()\n",
    "    slim.model_analyzer.analyze_vars(model_vars, print_info=True)\n",
    "\n",
    "model_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jMVZpBEd9EO5"
   },
   "outputs": [],
   "source": [
    "labels = tf.placeholder(tf.float32, shape=(None,2))\n",
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "acc_value = tf.reduce_mean(accuracy(labels, output))\n",
    "# correct_pred = tf.equal(output, labels)\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "from keras.objectives import categorical_crossentropy\n",
    "loss = tf.reduce_mean(categorical_crossentropy(labels, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_train.npy\")\n",
    "y_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_train_labels.npy\")\n",
    "y_train = to_categorical(y_train)\n",
    "X_dev_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_train.npy\")\n",
    "X_dev_val = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_val.npy\")\n",
    "y_dev_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_train_labels.npy\")\n",
    "y_dev_val = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_val_labels.npy\")\n",
    "# dev_wpt_levels_data_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_wpt_levels_data_train.npy\")\n",
    "# dev_wpt_levels_data_val = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_wpt_levels_data_val.npy\")\n",
    "# wpt_levels_data_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_wpt_levels_data_train.npy\")\n",
    "X_val = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_val.npy\")\n",
    "y_val = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_val_labels.npy\")\n",
    "y_val = to_categorical(y_val)\n",
    "# wpt_levels_data_val = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_wpt_levels_data_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dev_wpt_levels_data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ab331591e1c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdev_wpt_levels_data_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_wpt_levels_data_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwpt_levels_data_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_wpt_levels_data_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_wpt_levels_data_train' is not defined"
     ]
    }
   ],
   "source": [
    "dev_wpt_levels_data_train.shape,dev_wpt_levels_data_val.shape,wpt_levels_data_train.shape,dev_wpt_levels_data_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging dev_train and dev_val with x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.concatenate([X_train,X_dev_train,X_dev_val])\n",
    "y_train=np.concatenate([y_train,to_categorical(y_dev_train),to_categorical(y_dev_val)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "SYedzAue9ERm",
    "outputId": "18c403b6-383f-46bf-cdd7-76e009fb4b07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "((128, 3200, 1), (128, 3200, 1), (128, 2))\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_l1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-cbbe98b005a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_l1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#             print(sess.run(output,feed_dict))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_l1' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "# X_batch = np.array((batch_size,75673))\n",
    "# y_train = np.array((batch_size))\n",
    "train_step = tf.train.RMSPropOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "step = 0\n",
    "# Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(1):\n",
    "        for X_batch, wpt, y_batch in generator(X_train, y_train, 128):\n",
    "            step = step + 1\n",
    "            print(step)\n",
    "            print(X_batch.shape,wpt.shape,y_batch.shape)\n",
    "            feed_dict = {input_: X_batch, labels: y_batch, input_l1: wpt, K.learning_phase(): 1}\n",
    "            sess.run(train_step,feed_dict)\n",
    "#             print(sess.run(output,feed_dict))\n",
    "            if step % 1 == 0:\n",
    "                loss_val,acc_val = (sess.run([loss,acc_value],feed_dict))\n",
    "                print(\"Epoch: \"+str(i)+\"step: \"+str(step)+\"Training loss: \",loss_val,\" \",\"Training accuracy\",\" \",acc_val)\n",
    "                loss_val,acc_val = (sess.run([loss,acc_value],feed_dict={input_: X_batch, labels: y_batch, input_l1: wpt, K.learning_phase(): 0}))\n",
    "                print(\"Epoch: \"+str(i)+\"step: \"+str(step)+\"loss: \",loss_val,\" \",\"accuracy\",\" \",acc_val)\n",
    "    for X_batch, wpt, y_batch in generator(X_val, y_val, 128):\n",
    "        loss_val_val,acc_val_val = (sess.run([loss,acc_value],feed_dict={input_: X_batch, labels: y_batch, input_l1: wpt, K.learning_phase(): 0}))\n",
    "        print(\"val loss: \",loss_val_val,\" \",\"val accuracy\",\" \",acc_val_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "# X_batch = np.array((batch_size,75673))\n",
    "# y_train = np.array((batch_size))\n",
    "train_step = tf.train.RMSPropOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# Initialize all variables\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "step = 0\n",
    "# Run training loop\n",
    "with sess.as_default():\n",
    "    for i in range(1):\n",
    "        for X_batch, y_batch in generator(X_train, y_train, 128):\n",
    "            step = step + 1\n",
    "            print(step)\n",
    "            print(X_batch.shape,y_batch.shape)\n",
    "            feed_dict = {input_: X_batch, labels: y_batch, K.learning_phase(): 1}\n",
    "            sess.run(train_step,feed_dict)\n",
    "#             print(sess.run(output,feed_dict))\n",
    "            loss_val,acc_val = (sess.run([loss,acc_value],feed_dict))\n",
    "            print(\"Epoch: \"+str(i)+\"step: \"+str(step)+\"Training loss: \",loss_val,\" \",\"Training accuracy\",\" \",acc_val)\n",
    "            loss_val,acc_val = (sess.run([loss,acc_value],feed_dict={input_: X_batch, labels: y_batch, K.learning_phase(): 0}))\n",
    "            print(\"Epoch: \"+str(i)+\"step: \"+str(step)+\"loss: \",loss_val,\" \",\"accuracy\",\" \",acc_val)\n",
    "            if step % 2000 == 0:\n",
    "                step_val = 0    \n",
    "                for X_batch, y_batch in generator_val(X_val, y_val, 128):\n",
    "                    step_val = step_val + 1\n",
    "                    loss_val_val,acc_val_val = (sess.run([loss,acc_value],feed_dict={input_: X_batch, labels: y_batch, K.learning_phase(): 0}))\n",
    "                    print(\"val loss: \",loss_val_val,\" \",\"val accuracy\",\" \",acc_val_val)\n",
    "                    if step_val == 1000:\n",
    "                        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_train.npy\")\n",
    "y_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_train_labels.npy\")\n",
    "wpt_levels_data_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_wpt_levels_data_train.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IjXYvZGX6VYn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3611,), (3611, 4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, wpt_levels_data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L3R6e3kO9EWq",
    "outputId": "96b96120-9550-4d42-fd22-2a38a5711b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "\n",
    "acc_value = accuracy(labels, preds)\n",
    "with sess.as_default():\n",
    "    print(acc_value.eval(feed_dict={img: mnist_data.test.images,\n",
    "                                    labels: mnist_data.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qCjupTcM-gJF",
    "outputId": "43c4032f-0f54-4f2b-8277-7911858d296b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.chunking at 0x7f2d70eeaa58>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunking(X_train,y_train,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pjQBjVYNWkvE",
    "outputId": "408bcc88-5f9f-40df-beb5-4a2eed8f6a60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([0, 0, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.load(\"/content/drive/My Drive/SA/Code/spoof_detection_deep_features/WaveletCNN/temp_npy/labels_train.npy\",mmap_mode='r')\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0HzJnZ4e_PJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "WaveletCNN_Latest_ASVspoof2015.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
