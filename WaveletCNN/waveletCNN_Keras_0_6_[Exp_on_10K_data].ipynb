{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section to set gpu usage and create a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zQyX8ddJk32y",
    "outputId": "85d913ac-cc14-4195-f6f8-ef81b190f334"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import Adam\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical,Sequence\n",
    "import pandas as pd\n",
    "import math\n",
    "import sincnet\n",
    "from tensorflow import set_random_seed\n",
    "from keras import models, layers\n",
    "import numpy as np\n",
    "import sincnet\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import MaxPooling1D, Conv1D, LeakyReLU, BatchNormalization, Dense, Flatten\n",
    "from keras.layers import InputLayer, Input\n",
    "from keras.models import Model\n",
    "# import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to do framing and windowing over the audio to get a 2D output that can feeded to the WCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(signal):\n",
    "    pre_emphasis = 0.97\n",
    "    frame_size = 256\n",
    "    frame_stride = 128\n",
    "    nfilt = 20\n",
    "    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "    frame_length, frame_step = frame_size, frame_stride  # Convert from seconds to samples\n",
    "    signal_length = len(emphasized_signal)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n",
    "#     print(num_frames)\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    frames *= np.hamming(frame_length)\n",
    "    print(frames.shape)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section to perform wavelet decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMcF2AfPk_SX"
   },
   "outputs": [],
   "source": [
    "# batch operation usng tensor slice\n",
    "def WaveletTransformAxisY(batch_img):\n",
    "    odd_img  = batch_img[:,0::2]\n",
    "    even_img = batch_img[:,1::2]\n",
    "    L = (odd_img + even_img) / 2.0\n",
    "    H = K.abs(odd_img - even_img)\n",
    "    return L, H\n",
    "\n",
    "def WaveletTransformAxisX(batch_img):\n",
    "    # transpose + fliplr\n",
    "    tmp_batch = K.permute_dimensions(batch_img, [0, 2, 1])[:,:,::-1]\n",
    "    _dst_L, _dst_H = WaveletTransformAxisY(tmp_batch)\n",
    "    # transpose + flipud\n",
    "    dst_L = K.permute_dimensions(_dst_L, [0, 2, 1])[:,::-1,...]\n",
    "    dst_H = K.permute_dimensions(_dst_H, [0, 2, 1])[:,::-1,...]\n",
    "    return dst_L, dst_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WLXXxjIhlCvn"
   },
   "outputs": [],
   "source": [
    "def Wavelet(batch_image):\n",
    "    # make channel first image\n",
    "    batch_image = K.permute_dimensions(batch_image, [0, 3, 1, 2])\n",
    "    r = batch_image[:,0]\n",
    "    print(r.shape)\n",
    "\n",
    "    # level 1 decomposition\n",
    "    wavelet_L, wavelet_H = WaveletTransformAxisY(r)\n",
    "    r_wavelet_LL, r_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
    "    r_wavelet_HL, r_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
    "\n",
    "\n",
    "    wavelet_data = [r_wavelet_LL, r_wavelet_LH, r_wavelet_HL, r_wavelet_HH] \n",
    "#                    \n",
    "    transform_batch = K.stack(wavelet_data, axis=1)\n",
    "\n",
    "    # level 2 decomposition\n",
    "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(r_wavelet_LL)\n",
    "    r_wavelet_LL2, r_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
    "    r_wavelet_HL2, r_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
    "\n",
    "\n",
    "    wavelet_data_l2 = [r_wavelet_LL2, r_wavelet_LH2, r_wavelet_HL2, r_wavelet_HH2]\n",
    "#                     \n",
    "    transform_batch_l2 = K.stack(wavelet_data_l2, axis=1)\n",
    "\n",
    "    # level 3 decomposition\n",
    "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(r_wavelet_LL2)\n",
    "    r_wavelet_LL3, r_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
    "    r_wavelet_HL3, r_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
    "\n",
    "\n",
    "    wavelet_data_l3 = [r_wavelet_LL3, r_wavelet_LH3, r_wavelet_HL3, r_wavelet_HH3]\n",
    "#                     \n",
    "    transform_batch_l3 = K.stack(wavelet_data_l3, axis=1)\n",
    "\n",
    "    # level 4 decomposition\n",
    "    wavelet_L4, wavelet_H4 = WaveletTransformAxisY(r_wavelet_LL3)\n",
    "    r_wavelet_LL4, r_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
    "    r_wavelet_HL4, r_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
    "\n",
    "\n",
    "\n",
    "    wavelet_data_l4 = [r_wavelet_LL4, r_wavelet_LH4, r_wavelet_HL4, r_wavelet_HH4]\n",
    "#                     \n",
    "    transform_batch_l4 = K.stack(wavelet_data_l4, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    decom_level_1 = K.permute_dimensions(transform_batch, [0, 2, 3, 1])\n",
    "    decom_level_2 = K.permute_dimensions(transform_batch_l2, [0, 2, 3, 1])\n",
    "    decom_level_3 = K.permute_dimensions(transform_batch_l3, [0, 2, 3, 1])\n",
    "    decom_level_4 = K.permute_dimensions(transform_batch_l4, [0, 2, 3, 1])\n",
    "    \n",
    "\n",
    "    return [decom_level_1, decom_level_2, decom_level_3, decom_level_4]\n",
    "\n",
    "\n",
    "def Wavelet_out_shape(input_shapes):\n",
    "    # print('in to shape')\n",
    "    return [tuple([None, 296, 128, 4]), tuple([None, 148, 64, 4]), \n",
    "            tuple([None, 74, 32, 4]), tuple([None, 37, 16, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6dmgkdTlDOk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 592, 256)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'transpose_54:0' shape=(8, 296, 128, 4) dtype=float32>,\n",
       " <tf.Tensor 'transpose_55:0' shape=(8, 148, 64, 4) dtype=float32>,\n",
       " <tf.Tensor 'transpose_56:0' shape=(8, 74, 32, 4) dtype=float32>,\n",
       " <tf.Tensor 'transpose_57:0' shape=(8, 37, 16, 4) dtype=float32>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch = K.zeros(shape=(8, 592, 256, 1), dtype='float32')\n",
    "Wavelet(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "im_batch = K.zeros(shape=(8, 224, 224, 3), dtype='float32')\n",
    "im_batch = K.permute_dimensions(im_batch, [0, 3, 1, 2])\n",
    "r = im_batch[:,0]\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet CNN model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2MpxaRCZlF1J"
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "\tinitial_lrate = 0.1\n",
    "\tdrop = 0.5\n",
    "\tepochs_drop = 10.0\n",
    "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "\treturn lrate\n",
    "\n",
    "def get_wavelet_cnn_model():\n",
    "\n",
    "    input_shape = 75673, 1\n",
    "\n",
    "    input_ = Input(input_shape, name='the_input')\n",
    "    \n",
    "    x = sincnet.SincConv1D(80, 251, 16000)(input_)\n",
    "    x = MaxPooling1D(pool_size=3)(x)\n",
    "    x = BatchNormalization(momentum=0.05)(x)\n",
    "    x = sincnet.LayerNorm()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    # wavelet = Lambda(Wavelet, name='wavelet')\n",
    "    wavelet = Lambda(Wavelet, Wavelet_out_shape, name='wavelet')\n",
    "    input_l1, input_l2, input_l3, input_l4 = wavelet(x)\n",
    "    print(input_l1.shape)\n",
    "    # print(input_l2)\n",
    "    # print(input_l3)\n",
    "    # print(input_l4)\n",
    "    # level one decomposition starts\n",
    "    conv_1 = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_1')(input_l1)\n",
    "    norm_1 = BatchNormalization(name='norm_1')(conv_1)\n",
    "    relu_1 = Activation('relu', name='relu_1')(norm_1)\n",
    "\n",
    "    conv_1_2 = Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_1_2')(relu_1)\n",
    "    norm_1_2 = BatchNormalization(name='norm_1_2')(conv_1_2)\n",
    "    relu_1_2 = Activation('relu', name='relu_1_2')(norm_1_2)\n",
    "\n",
    "    # level two decomposition starts\n",
    "    conv_a = Conv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv_a')(input_l2)\n",
    "    norm_a = BatchNormalization(name='norm_a')(conv_a)\n",
    "    relu_a = Activation('relu', name='relu_a')(norm_a)\n",
    "\n",
    "    # concate level one and level two decomposition\n",
    "    concate_level_2 = concatenate([relu_1_2, relu_a])\n",
    "    conv_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_2')(concate_level_2)\n",
    "    norm_2 = BatchNormalization(name='norm_2')(conv_2)\n",
    "    relu_2 = Activation('relu', name='relu_2')(norm_2)\n",
    "\n",
    "    conv_2_2 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_2_2')(relu_2)\n",
    "    norm_2_2 = BatchNormalization(name='norm_2_2')(conv_2_2)\n",
    "    relu_2_2 = Activation('relu', name='relu_2_2')(norm_2_2)\n",
    "\n",
    "    # level three decomposition starts \n",
    "    conv_b = Conv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv_b')(input_l3)\n",
    "    norm_b = BatchNormalization(name='norm_b')(conv_b)\n",
    "    relu_b = Activation('relu', name='relu_b')(norm_b)\n",
    "\n",
    "    conv_b_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_b_2')(relu_b)\n",
    "    norm_b_2 = BatchNormalization(name='norm_b_2')(conv_b_2)\n",
    "    relu_b_2 = Activation('relu', name='relu_b_2')(norm_b_2)\n",
    "\n",
    "    # concate level two and level three decomposition \n",
    "    concate_level_3 = concatenate([relu_2_2, relu_b_2])\n",
    "    conv_3 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_3')(concate_level_3)\n",
    "    norm_3 = BatchNormalization(name='nomr_3')(conv_3)\n",
    "    relu_3 = Activation('relu', name='relu_3')(norm_3)\n",
    "\n",
    "    conv_3_2 = Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_3_2')(relu_3)\n",
    "    norm_3_2 = BatchNormalization(name='norm_3_2')(conv_3_2)\n",
    "    relu_3_2 = Activation('relu', name='relu_3_2')(norm_3_2)\n",
    "\n",
    "    # level four decomposition start\n",
    "    conv_c = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_c')(input_l4)\n",
    "    norm_c = BatchNormalization(name='norm_c')(conv_c)\n",
    "    relu_c = Activation('relu', name='relu_c')(norm_c)\n",
    "\n",
    "    conv_c_2 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_c_2')(relu_c)\n",
    "    norm_c_2 = BatchNormalization(name='norm_c_2')(conv_c_2)\n",
    "    relu_c_2 = Activation('relu', name='relu_c_2')(norm_c_2)\n",
    "\n",
    "    conv_c_3 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_c_3')(relu_c_2)\n",
    "    norm_c_3 = BatchNormalization(name='norm_c_3')(conv_c_3)\n",
    "    relu_c_3 = Activation('relu', name='relu_c_3')(norm_c_3)\n",
    "\n",
    "    # concate level level three and level four decomposition\n",
    "    concate_level_4 = concatenate([relu_3_2, relu_c_3])\n",
    "    conv_4 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_4')(concate_level_4)\n",
    "    norm_4 = BatchNormalization(name='norm_4')(conv_4)\n",
    "    relu_4 = Activation('relu', name='relu_4')(norm_4)\n",
    "\n",
    "    conv_4_2 = Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_4_2')(relu_4)\n",
    "    norm_4_2 = BatchNormalization(name='norm_4_2')(conv_4_2)\n",
    "    relu_4_2 = Activation('relu', name='relu_4_2')(norm_4_2)\n",
    "\n",
    "    conv_5_1 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_5_1')(relu_4_2)\n",
    "    norm_5_1 = BatchNormalization(name='norm_5_1')(conv_5_1)\n",
    "    relu_5_1 = Activation('relu', name='relu_5_1')(norm_5_1)\n",
    "\n",
    "    pool_5_1 = AveragePooling2D(pool_size=(3, 3), strides=1, padding='same', name='avg_pool_5_1')(relu_5_1)\n",
    "    flat_5_1 = Flatten(name='flat_5_1')(pool_5_1) \n",
    "\n",
    "    fc_5 = Dense(2048, name='fc_5')(flat_5_1)\n",
    "    norm_5 = BatchNormalization(name='norm_5')(fc_5)\n",
    "    relu_5 = Activation('relu', name='relu_5')(norm_5)\n",
    "    drop_5 = Dropout(0.5, name='drop_5')(relu_5)\n",
    "\n",
    "    fc_6 = Dense(2048, name='fc_6')(drop_5)\n",
    "    norm_6 = BatchNormalization(name='norm_6')(fc_6)\n",
    "    relu_6 = Activation('relu', name='relu_6')(norm_6)\n",
    "    drop_6 = Dropout(0.5, name='drop_6')(relu_6)\n",
    "\n",
    "    output = Dense(2, activation='softmax', name='fc_7')(drop_6)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "    model.summary()\n",
    "#     plot_model(model, to_file='wavelet_cnn_0.5.png')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XlNN-MeWlQHX",
    "outputId": "729d4720-1ed6-47d9-d3c6-21c4f7008202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 592, 256)\n",
      "(?, 296, 128, 4)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 592, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "wavelet (Lambda)                [(None, 296, 128, 4) 0           the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 296, 128, 64) 2368        wavelet[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_1 (BatchNormalization)     (None, 296, 128, 64) 256         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_1 (Activation)             (None, 296, 128, 64) 0           norm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_1_2 (Conv2D)               (None, 148, 64, 64)  36928       relu_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_a (Conv2D)                 (None, 148, 64, 64)  2368        wavelet[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_1_2 (BatchNormalization)   (None, 148, 64, 64)  256         conv_1_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_a (BatchNormalization)     (None, 148, 64, 64)  256         conv_a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_1_2 (Activation)           (None, 148, 64, 64)  0           norm_1_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_a (Activation)             (None, 148, 64, 64)  0           norm_a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 148, 64, 128) 0           relu_1_2[0][0]                   \n",
      "                                                                 relu_a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 148, 64, 128) 147584      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_b (Conv2D)                 (None, 74, 32, 64)   2368        wavelet[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_2 (BatchNormalization)     (None, 148, 64, 128) 512         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_b (BatchNormalization)     (None, 74, 32, 64)   256         conv_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_2 (Activation)             (None, 148, 64, 128) 0           norm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_b (Activation)             (None, 74, 32, 64)   0           norm_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2_2 (Conv2D)               (None, 74, 32, 128)  147584      relu_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_b_2 (Conv2D)               (None, 74, 32, 128)  73856       relu_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_2_2 (BatchNormalization)   (None, 74, 32, 128)  512         conv_2_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_b_2 (BatchNormalization)   (None, 74, 32, 128)  512         conv_b_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_c (Conv2D)                 (None, 37, 16, 64)   2368        wavelet[0][3]                    \n",
      "__________________________________________________________________________________________________\n",
      "relu_2_2 (Activation)           (None, 74, 32, 128)  0           norm_2_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_b_2 (Activation)           (None, 74, 32, 128)  0           norm_b_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_c (BatchNormalization)     (None, 37, 16, 64)   256         conv_c[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 74, 32, 256)  0           relu_2_2[0][0]                   \n",
      "                                                                 relu_b_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_c (Activation)             (None, 37, 16, 64)   0           norm_c[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 74, 32, 256)  590080      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_c_2 (Conv2D)               (None, 37, 16, 256)  147712      relu_c[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "nomr_3 (BatchNormalization)     (None, 74, 32, 256)  1024        conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_c_2 (BatchNormalization)   (None, 37, 16, 256)  1024        conv_c_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_3 (Activation)             (None, 74, 32, 256)  0           nomr_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_c_2 (Activation)           (None, 37, 16, 256)  0           norm_c_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_3_2 (Conv2D)               (None, 37, 16, 256)  590080      relu_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_c_3 (Conv2D)               (None, 37, 16, 256)  590080      relu_c_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_3_2 (BatchNormalization)   (None, 37, 16, 256)  1024        conv_3_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_c_3 (BatchNormalization)   (None, 37, 16, 256)  1024        conv_c_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_3_2 (Activation)           (None, 37, 16, 256)  0           norm_3_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_c_3 (Activation)           (None, 37, 16, 256)  0           norm_c_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 37, 16, 512)  0           relu_3_2[0][0]                   \n",
      "                                                                 relu_c_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 37, 16, 256)  1179904     concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "norm_4 (BatchNormalization)     (None, 37, 16, 256)  1024        conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_4 (Activation)             (None, 37, 16, 256)  0           norm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4_2 (Conv2D)               (None, 19, 8, 256)   590080      relu_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_4_2 (BatchNormalization)   (None, 19, 8, 256)   1024        conv_4_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_4_2 (Activation)           (None, 19, 8, 256)   0           norm_4_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_5_1 (Conv2D)               (None, 19, 8, 128)   295040      relu_4_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_5_1 (BatchNormalization)   (None, 19, 8, 128)   512         conv_5_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_5_1 (Activation)           (None, 19, 8, 128)   0           norm_5_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool_5_1 (AveragePooling2D) (None, 19, 8, 128)   0           relu_5_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flat_5_1 (Flatten)              (None, 19456)        0           avg_pool_5_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_5 (Dense)                    (None, 2048)         39847936    flat_5_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_5 (BatchNormalization)     (None, 2048)         8192        fc_5[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "relu_5 (Activation)             (None, 2048)         0           norm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_5 (Dropout)                (None, 2048)         0           relu_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fc_6 (Dense)                    (None, 2048)         4196352     drop_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_6 (BatchNormalization)     (None, 2048)         8192        fc_6[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "relu_6 (Activation)             (None, 2048)         0           norm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_6 (Dropout)                (None, 2048)         0           relu_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fc_7 (Dense)                    (None, 2)            4098        drop_6[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 48,472,642\n",
      "Trainable params: 48,459,714\n",
      "Non-trainable params: 12,928\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_wavelet_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "w6zpGp6flIEz",
    "outputId": "25b3b88a-b5f9-42d1-c769-8d46517c3bd7"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chunking_framed_windowed(Sequence):\n",
    "    def __init__(self, data_train, train_labels, batch_size):\n",
    "        self.data_train = data_train\n",
    "        self.train_labels = train_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.n = 0\n",
    "        self.max = self.__len__()\n",
    "        self.train_batch = []\n",
    "    def __len__(self):\n",
    "        return np.ceil(len(self.data_train) / float(self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.data_train[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.train_labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#         print(batch_x[1],idx)\n",
    "        self.train_batch = []\n",
    "        for i in batch_x:\n",
    "            self.train_batch.append(np.array(test(i)))\n",
    "#         print(len(self.window_train),len(batch_y))\n",
    "        return np.array(self.train_batch), np.array(batch_y)\n",
    "    def __next__(self):\n",
    "        if self.n >= self.max:\n",
    "            self.n = 0\n",
    "        result = self.__getitem__(self.n)\n",
    "        self.n += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section to import labels and trimmed audio of ASVSpoof 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/train_label.npy\")\n",
    "trimmed_audio = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/trimmed_audio.npy\", mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-60b8e8eaaf46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_labels1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'human'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_labels' is not defined"
     ]
    }
   ],
   "source": [
    "train_labels=list(train_labels)\n",
    "\n",
    "train_labels1 = list()\n",
    "for i in train_labels:\n",
    "    if i == b'human':\n",
    "        train_labels1.append(1)\n",
    "    else:\n",
    "        train_labels1.append(0)\n",
    "train_labels = to_categorical(train_labels1[:16000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do framing and windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framed_windowed=[]\n",
    "for i in trimmed_audio:\n",
    "    framed_windowed.append(test(i))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed.npy\",framed_windowed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/train_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "framed_windowed=framed_windowed.reshape(16000,590,256,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting the dimensions of the windowed dataset to perform wavelet transformation properly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "framed_windowed_new = []\n",
    "for count,i in enumerate(framed_windowed):\n",
    "    framed_windowed_new.append(np.pad(i,((0,2),(0,0),(0,0)),'constant'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "framed_windowed_new = np.array(framed_windowed_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "framed_windowed = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed.npy\",framed_windowed_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split dataset for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(framed_windowed, train_labels, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed_train.npy\",X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed_val.npy\",X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed_labels_train.npy\",y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed_labels_val.npy\",y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed_train.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed_labels_train.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed_val.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/framed_windowed_labels_val.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11200, 592, 256, 1), (4800, 592, 256, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ejmciqicnMnu",
    "outputId": "ebacd6d9-2162-4215-da3c-b4afca30a00d"
   },
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(\n",
    "\t#rescale=1./255,\n",
    "\tshear_range=0.1,\n",
    "\tzoom_range=0.1,\n",
    "\thorizontal_flip=True\n",
    ")\n",
    "val_data_gen = ImageDataGenerator(\n",
    "\t#rescale=1./255\n",
    ")\n",
    "\n",
    "\n",
    "# def train & test generators\n",
    "train_generator = train_data_gen.flow(\n",
    "    X_train,\n",
    "    y_train,\n",
    "\tbatch_size=32)\n",
    "val_generator = val_data_gen.flow(\n",
    "\tX_val,\n",
    "\ty_val,\n",
    "    batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "colab_type": "code",
    "id": "V2740Mdynf0w",
    "outputId": "bfc3b627-135e-436e-ac02-82838f4e7ea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "350/350 [==============================] - 101s 287ms/step - loss: 3.7597 - acc: 0.7651 - val_loss: 3.5527 - val_acc: 0.7796\n",
      "Epoch 2/20\n",
      "350/350 [==============================] - 99s 284ms/step - loss: 3.7590 - acc: 0.7668 - val_loss: 3.5829 - val_acc: 0.7777\n",
      "Epoch 3/20\n",
      "350/350 [==============================] - 99s 284ms/step - loss: 3.7647 - acc: 0.7664 - val_loss: 3.5393 - val_acc: 0.7804\n",
      "Epoch 4/20\n",
      "350/350 [==============================] - 99s 284ms/step - loss: 3.7618 - acc: 0.7666 - val_loss: 3.5762 - val_acc: 0.7781\n",
      "Epoch 5/20\n",
      "350/350 [==============================] - 99s 284ms/step - loss: 3.7633 - acc: 0.7665 - val_loss: 3.4285 - val_acc: 0.7873\n",
      "Epoch 6/20\n",
      "350/350 [==============================] - 100s 284ms/step - loss: 3.7719 - acc: 0.7660 - val_loss: 3.6366 - val_acc: 0.7744\n",
      "Epoch 7/20\n",
      "350/350 [==============================] - 99s 284ms/step - loss: 3.7604 - acc: 0.7667 - val_loss: 3.5829 - val_acc: 0.7777\n",
      "Epoch 8/20\n",
      "350/350 [==============================] - 100s 285ms/step - loss: 3.7676 - acc: 0.7662 - val_loss: 3.4990 - val_acc: 0.7829\n",
      "Epoch 9/20\n",
      "350/350 [==============================] - 99s 284ms/step - loss: 3.7719 - acc: 0.7660 - val_loss: 3.4822 - val_acc: 0.7840\n",
      "Epoch 10/20\n",
      "350/350 [==============================] - 99s 284ms/step - loss: 3.7618 - acc: 0.7666 - val_loss: 3.6501 - val_acc: 0.7735\n",
      "Epoch 11/20\n",
      "350/350 [==============================] - 100s 285ms/step - loss: 3.7575 - acc: 0.7669 - val_loss: 3.5561 - val_acc: 0.7794\n",
      "Epoch 12/20\n",
      "350/350 [==============================] - 100s 285ms/step - loss: 3.7633 - acc: 0.7665 - val_loss: 3.5292 - val_acc: 0.7810\n",
      "Epoch 13/20\n",
      "350/350 [==============================] - 99s 284ms/step - loss: 3.7705 - acc: 0.7661 - val_loss: 3.6232 - val_acc: 0.7752\n",
      "Epoch 14/20\n",
      "350/350 [==============================] - 99s 284ms/step - loss: 3.7618 - acc: 0.7666 - val_loss: 3.5561 - val_acc: 0.7794\n",
      "Epoch 15/20\n",
      "350/350 [==============================] - 99s 284ms/step - loss: 3.7690 - acc: 0.7662 - val_loss: 3.5191 - val_acc: 0.7817\n",
      "Epoch 16/20\n",
      " 76/350 [=====>........................] - ETA: 1:08 - loss: 3.8307 - acc: 0.7623"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-71d4c3f8876b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4800\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \tverbose=True)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2145\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2146\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2147\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1837\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "\ttrain_generator,\n",
    "\tsteps_per_epoch=11200//32,\n",
    "\tepochs=200,\n",
    "\tvalidation_data=val_generator,\n",
    "    validation_steps=4800//32,\n",
    "    callbacks=callbacks_list,\n",
    "\tverbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "MI3d6ONirH6h",
    "outputId": "2c7ff91a-14dc-46c3-bd45-d2e3d00d7dab"
   },
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(acc, label='training accuracy')\n",
    "plt.plot(val_acc, label='validation accuracy')\n",
    "plt.title('Accuracy curve')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "FjlFssvfrIwO",
    "outputId": "f1bfef36-bc05-4740-fd73-4168d19762ad"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(loss, label='training loss')\n",
    "plt.plot(val_loss, label='validation loss')\n",
    "plt.title('Loss curve')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(input_shape,out_dim):\n",
    "    #\n",
    "    inputs = Input(input_shape)\n",
    "#     print(inputs)\n",
    "    x = sincnet.SincConv1D(80, 251, 16000)(inputs)\n",
    "\n",
    "\n",
    "    x = MaxPooling1D(pool_size=3)(x)\n",
    "    x = BatchNormalization(momentum=0.05)(x)\n",
    "    x = sincnet.LayerNorm()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    #DNN final\n",
    "    prediction = layers.Dense(out_dim, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=prediction)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 75673, 1)          0         \n",
      "_________________________________________________________________\n",
      "sinc_conv1d_5 (SincConv1D)   (None, 75423, 80)         160       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 25141, 80)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 25141, 80)         320       \n",
      "_________________________________________________________________\n",
      "layer_norm_2 (LayerNorm)     (None, 25141, 80)         160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 25141, 80)         0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25141, 2)          162       \n",
      "=================================================================\n",
      "Total params: 802\n",
      "Trainable params: 642\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input_shape = (n,1)\n",
    "model = getModel((trimmed_audio[1].shape[0],1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75673,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed_audio[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": " waveletCNN_Keras_0.6_[Exp on 10K_data].ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
