{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section to set gpu usage and create a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/07/4b361d6d0f4e08942575f83a11d33f36897e1aae4279046606dd1808778a/matplotlib-3.1.3-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
      "\u001b[K     |████████████████████████████████| 13.1MB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11 in /home/rohit/anaconda3/envs/EEG/lib/python3.6/site-packages (from matplotlib) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/rohit/anaconda3/envs/EEG/lib/python3.6/site-packages (from matplotlib) (2.8.0)\n",
      "Collecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 2.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/bc/1e58593167fade7b544bfe9502a26dc860940a79ab306e651e7f13be68c2/pyparsing-2.4.6-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 2.0MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.5 in /home/rohit/anaconda3/envs/EEG/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/rohit/anaconda3/envs/EEG/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.2.0)\n",
      "Installing collected packages: kiwisolver, pyparsing, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.3 pyparsing-2.4.6\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zQyX8ddJk32y",
    "outputId": "85d913ac-cc14-4195-f6f8-ef81b190f334"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import AveragePooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import plot_model\n",
    "from keras.utils import to_categorical,Sequence\n",
    "import pandas as pd\n",
    "import math\n",
    "import sincnet\n",
    "from tensorflow import set_random_seed\n",
    "from keras import models, layers\n",
    "import numpy as np\n",
    "import sincnet\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import MaxPooling1D,MaxPooling2D, Conv1D, LeakyReLU, BatchNormalization, Dense, Flatten\n",
    "from keras.layers import InputLayer, Input\n",
    "from keras.models import Model\n",
    "# from tensorflow.python.keras.utils.data_utils import Sequence\n",
    "# import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to do framing and windowing over the audio to get a 2D output that can feeded to the WCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(signal):\n",
    "    pre_emphasis = 0.97\n",
    "    frame_size = 256\n",
    "    frame_stride = 128\n",
    "    nfilt = 20\n",
    "    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "    frame_length, frame_step = frame_size, frame_stride  # Convert from seconds to samples\n",
    "    signal_length = len(emphasized_signal)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))  # Make sure that we have at least 1 frame\n",
    "#     print(num_frames)\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(emphasized_signal, z) # Pad Signal to make sure that all frames have equal number of samples without truncating any samples from the original signal\n",
    "\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    frames *= np.hamming(frame_length)\n",
    "    print(frames.shape)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section to perform wavelet decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMcF2AfPk_SX"
   },
   "outputs": [],
   "source": [
    "# batch operation usng tensor slice\n",
    "def WaveletTransformAxisY(batch_img):\n",
    "    odd_img  = batch_img[:,0::2]\n",
    "    even_img = batch_img[:,1::2]\n",
    "    L = (odd_img + even_img) / 2.0\n",
    "    H = K.abs(odd_img - even_img)\n",
    "    return L, H\n",
    "\n",
    "def WaveletTransformAxisX(batch_img):\n",
    "    # transpose + fliplr\n",
    "    tmp_batch = K.permute_dimensions(batch_img, [0, 2, 1])[:,:,::-1]\n",
    "    _dst_L, _dst_H = WaveletTransformAxisY(tmp_batch)\n",
    "    # transpose + flipud\n",
    "    dst_L = K.permute_dimensions(_dst_L, [0, 2, 1])[:,::-1,...]\n",
    "    dst_H = K.permute_dimensions(_dst_H, [0, 2, 1])[:,::-1,...]\n",
    "    return dst_L, dst_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WLXXxjIhlCvn"
   },
   "outputs": [],
   "source": [
    "def Wavelet(batch_image):\n",
    "    # make channel first image\n",
    "    batch_image = K.permute_dimensions(batch_image, [0, 3, 1, 2])\n",
    "    r = batch_image[:,0]\n",
    "    print(r.shape)\n",
    "\n",
    "    # level 1 decomposition\n",
    "    wavelet_L, wavelet_H = WaveletTransformAxisY(r)\n",
    "    r_wavelet_LL, r_wavelet_LH = WaveletTransformAxisX(wavelet_L)\n",
    "    r_wavelet_HL, r_wavelet_HH = WaveletTransformAxisX(wavelet_H)\n",
    "\n",
    "\n",
    "    wavelet_data = [r_wavelet_LL, r_wavelet_LH, r_wavelet_HL, r_wavelet_HH] \n",
    "#                    \n",
    "    transform_batch = K.stack(wavelet_data, axis=1)\n",
    "\n",
    "    # level 2 decomposition\n",
    "    wavelet_L2, wavelet_H2 = WaveletTransformAxisY(r_wavelet_LL)\n",
    "    r_wavelet_LL2, r_wavelet_LH2 = WaveletTransformAxisX(wavelet_L2)\n",
    "    r_wavelet_HL2, r_wavelet_HH2 = WaveletTransformAxisX(wavelet_H2)\n",
    "\n",
    "\n",
    "    wavelet_data_l2 = [r_wavelet_LL2, r_wavelet_LH2, r_wavelet_HL2, r_wavelet_HH2]\n",
    "#                     \n",
    "    transform_batch_l2 = K.stack(wavelet_data_l2, axis=1)\n",
    "\n",
    "    # level 3 decomposition\n",
    "    wavelet_L3, wavelet_H3 = WaveletTransformAxisY(r_wavelet_LL2)\n",
    "    r_wavelet_LL3, r_wavelet_LH3 = WaveletTransformAxisX(wavelet_L3)\n",
    "    r_wavelet_HL3, r_wavelet_HH3 = WaveletTransformAxisX(wavelet_H3)\n",
    "\n",
    "\n",
    "    wavelet_data_l3 = [r_wavelet_LL3, r_wavelet_LH3, r_wavelet_HL3, r_wavelet_HH3]\n",
    "#                     \n",
    "    transform_batch_l3 = K.stack(wavelet_data_l3, axis=1)\n",
    "\n",
    "    # level 4 decomposition\n",
    "    wavelet_L4, wavelet_H4 = WaveletTransformAxisY(r_wavelet_LL3)\n",
    "    r_wavelet_LL4, r_wavelet_LH4 = WaveletTransformAxisX(wavelet_L4)\n",
    "    r_wavelet_HL4, r_wavelet_HH4 = WaveletTransformAxisX(wavelet_H4)\n",
    "\n",
    "\n",
    "\n",
    "    wavelet_data_l4 = [r_wavelet_LL4, r_wavelet_LH4, r_wavelet_HL4, r_wavelet_HH4]\n",
    "#                     \n",
    "    transform_batch_l4 = K.stack(wavelet_data_l4, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    decom_level_1 = K.permute_dimensions(transform_batch, [0, 2, 3, 1])\n",
    "    decom_level_2 = K.permute_dimensions(transform_batch_l2, [0, 2, 3, 1])\n",
    "    decom_level_3 = K.permute_dimensions(transform_batch_l3, [0, 2, 3, 1])\n",
    "    decom_level_4 = K.permute_dimensions(transform_batch_l4, [0, 2, 3, 1])\n",
    "    \n",
    "\n",
    "    return [decom_level_1, decom_level_2, decom_level_3, decom_level_4]\n",
    "\n",
    "\n",
    "def Wavelet_out_shape(input_shapes):\n",
    "    # print('in to shape')\n",
    "    return [tuple([None, 12576, 40, 4]), tuple([None, 6288, 20, 4]), \n",
    "            tuple([None, 3144, 10, 4]), tuple([None, 1572, 5, 4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h6dmgkdTlDOk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 25152, 80)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'transpose_25:0' shape=(8, 12576, 40, 4) dtype=float32>,\n",
       " <tf.Tensor 'transpose_26:0' shape=(8, 6288, 20, 4) dtype=float32>,\n",
       " <tf.Tensor 'transpose_27:0' shape=(8, 3144, 10, 4) dtype=float32>,\n",
       " <tf.Tensor 'transpose_28:0' shape=(8, 1572, 5, 4) dtype=float32>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_batch = K.zeros(shape=(8,25152,80, 1), dtype='float32')\n",
    "Wavelet(img_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "im_batch = K.zeros(shape=(8, 224, 224, 3), dtype='float32')\n",
    "im_batch = K.permute_dimensions(im_batch, [0, 3, 1, 2])\n",
    "r = im_batch[:,0]\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet CNN model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2MpxaRCZlF1J"
   },
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "\tinitial_lrate = 0.1\n",
    "\tdrop = 0.5\n",
    "\tepochs_drop = 10.0\n",
    "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "\treturn lrate\n",
    "\n",
    "def get_wavelet_cnn_model():\n",
    "\n",
    "    input_shape = 75673, 1\n",
    "\n",
    "    input_ = Input(input_shape, name='the_input')\n",
    "\n",
    "    wavelet = Lambda(Wavelet, Wavelet_out_shape, name='wavelet')\n",
    "    input_l1, input_l2, input_l3, input_l4 = wavelet(input_)\n",
    "    print(input_l1.shape)\n",
    "    # print(input_l2)\n",
    "    # print(input_l3)\n",
    "    # print(input_l4)\n",
    "    # level one decomposition starts\n",
    "    conv_1 = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_1')(input_l1)\n",
    "#     pool_1 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_1)\n",
    "    norm_1 = BatchNormalization(name='norm_1')(conv_1)\n",
    "    relu_1 = Activation('relu', name='relu_1')(norm_1)\n",
    "\n",
    "    conv_1_2 = Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_1_2')(relu_1)\n",
    "    # pool_1_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_1_2)\n",
    "    norm_1_2 = BatchNormalization(name='norm_1_2')(conv_1_2)\n",
    "    relu_1_2 = Activation('relu', name='relu_1_2')(norm_1_2)\n",
    "    \n",
    "\n",
    "    # level two decomposition starts\n",
    "    conv_a = Conv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv_a')(input_l2)\n",
    "    # pool_a = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_a)\n",
    "    norm_a = BatchNormalization(name='norm_a')(conv_a)\n",
    "    relu_a = Activation('relu', name='relu_a')(norm_a)\n",
    "    print(relu_1_2.shape,relu_a.shape)\n",
    "    \n",
    "    # concate level one and level two decomposition\n",
    "    concate_level_2 = concatenate([relu_1_2, relu_a])\n",
    "    conv_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_2')(concate_level_2)\n",
    "#     pool_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_2)\n",
    "    norm_2 = BatchNormalization(name='norm_2')(conv_2)\n",
    "    relu_2 = Activation('relu', name='relu_2')(norm_2)\n",
    "\n",
    "    conv_2_2 = Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_2_2')(relu_2)\n",
    "    # pool_2_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_2_2)\n",
    "    norm_2_2 = BatchNormalization(name='norm_2_2')(conv_2_2)\n",
    "    relu_2_2 = Activation('relu', name='relu_2_2')(norm_2_2)\n",
    "\n",
    "    # level three decomposition starts \n",
    "    conv_b = Conv2D(filters=64, kernel_size=(3, 3), padding='same', name='conv_b')(input_l3)\n",
    "    # pool_b = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_b)\n",
    "    norm_b = BatchNormalization(name='norm_b')(conv_b)\n",
    "    relu_b = Activation('relu', name='relu_b')(norm_b)\n",
    "\n",
    "    conv_b_2 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_b_2')(relu_b)\n",
    "    # pool_b_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_b_2)\n",
    "    norm_b_2 = BatchNormalization(name='norm_b_2')(conv_b_2)\n",
    "    relu_b_2 = Activation('relu', name='relu_b_2')(norm_b_2)\n",
    "    print(relu_2_2.shape,relu_b_2.shape)\n",
    "    # concate level two and level three decomposition \n",
    "    concate_level_3 = concatenate([relu_2_2, relu_b_2])\n",
    "    conv_3 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_3')(concate_level_3)\n",
    "#     pool_3 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_3)\n",
    "    norm_3 = BatchNormalization(name='nomr_3')(conv_3)\n",
    "    relu_3 = Activation('relu', name='relu_3')(norm_3)\n",
    "\n",
    "    conv_3_2 = Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_3_2')(relu_3)\n",
    "    # pool_3_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_3_2)\n",
    "    norm_3_2 = BatchNormalization(name='norm_3_2')(conv_3_2)\n",
    "    relu_3_2 = Activation('relu', name='relu_3_2')(norm_3_2)\n",
    "\n",
    "    # level four decomposition start\n",
    "    conv_c = Conv2D(64, kernel_size=(3, 3), padding='same', name='conv_c')(input_l4)\n",
    "    # pool_c = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_c)\n",
    "    norm_c = BatchNormalization(name='norm_c')(conv_c)\n",
    "    relu_c = Activation('relu', name='relu_c')(norm_c)\n",
    "\n",
    "    conv_c_2 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_c_2')(relu_c)\n",
    "    # pool_c_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_c_2)\n",
    "    norm_c_2 = BatchNormalization(name='norm_c_2')(conv_c_2)\n",
    "    relu_c_2 = Activation('relu', name='relu_c_2')(norm_c_2)\n",
    "\n",
    "    conv_c_3 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_c_3')(relu_c_2)\n",
    "    # pool_c_3 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_c_3)\n",
    "    norm_c_3 = BatchNormalization(name='norm_c_3')(conv_c_3)\n",
    "    relu_c_3 = Activation('relu', name='relu_c_3')(norm_c_3)\n",
    "    print(relu_3_2.shape,relu_c_3.shape)\n",
    "    # concate level level three and level four decomposition\n",
    "    concate_level_4 = concatenate([relu_3_2, relu_c_3])\n",
    "    conv_4 = Conv2D(256, kernel_size=(3, 3), padding='same', name='conv_4')(concate_level_4)\n",
    "    # pool_4 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_4)\n",
    "    norm_4 = BatchNormalization(name='norm_4')(conv_4)\n",
    "    relu_4 = Activation('relu', name='relu_4')(norm_4)\n",
    "\n",
    "    conv_4_2 = Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', name='conv_4_2')(relu_4)\n",
    "    # pool_4_2 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_4_2)\n",
    "    norm_4_2 = BatchNormalization(name='norm_4_2')(conv_4_2)\n",
    "    relu_4_2 = Activation('relu', name='relu_4_2')(norm_4_2)\n",
    "\n",
    "    conv_5_1 = Conv2D(128, kernel_size=(3, 3), padding='same', name='conv_5_1')(relu_4_2)\n",
    "    # pool_5_1 = MaxPooling2D(pool_size=(3,3),padding = 'same')(conv_5_1)\n",
    "    norm_5_1 = BatchNormalization(name='norm_5_1')(conv_5_1)\n",
    "    relu_5_1 = Activation('relu', name='relu_5_1')(norm_5_1)\n",
    "\n",
    "    pool_5_1 = AveragePooling2D(pool_size=(3, 3), strides=1, padding='same', name='avg_pool_5_1')(relu_5_1)\n",
    "    flat_5_1 = Flatten(name='flat_5_1')(pool_5_1) \n",
    "\n",
    "    fc_5 = Dense(2048, name='fc_5')(flat_5_1)\n",
    "    norm_5 = BatchNormalization(name='norm_5')(fc_5)\n",
    "    relu_5 = Activation('relu', name='relu_5')(norm_5)\n",
    "    drop_5 = Dropout(0.5, name='drop_5')(relu_5)\n",
    "\n",
    "    fc_6 = Dense(2048, name='fc_6')(drop_5)\n",
    "    norm_6 = BatchNormalization(name='norm_6')(fc_6)\n",
    "    relu_6 = Activation('relu', name='relu_6')(norm_6)\n",
    "    drop_6 = Dropout(0.5, name='drop_6')(relu_6)\n",
    "\n",
    "    output = Dense(2, activation='softmax', name='fc_7')(drop_6)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "    model.summary()\n",
    "#     plot_model(model, to_file='wavelet_cnn_0.5.png')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XlNN-MeWlQHX",
    "outputId": "729d4720-1ed6-47d9-d3c6-21c4f7008202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 25152, 80)\n",
      "(?, 12576, 40, 4)\n",
      "(?, 2096, 7, 64) (?, 2096, 7, 64)\n",
      "(?, 350, 2, 128) (?, 350, 2, 128)\n",
      "(?, 59, 1, 256) (?, 59, 1, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0220 20:52:48.279892 140650830956352 deprecation.py:506] From /home/rohit/anaconda3/envs/EEG/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "the_input (InputLayer)          (None, 75673, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sinc_conv1d_7 (SincConv1D)      (None, 75423, 80)    160         the_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 25141, 80)    0           sinc_conv1d_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 25141, 80)    320         max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "layer_norm_7 (LayerNorm)        (None, 25141, 80)    160         batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 25141, 80)    0           layer_norm_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 25152, 80)    0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 25152, 80, 1) 0           lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "wavelet (Lambda)                [(None, 12576, 40, 4 0           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, 12576, 40, 64 2368        wavelet[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "norm_1 (BatchNormalization)     (None, 12576, 40, 64 256         conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_1 (Activation)             (None, 12576, 40, 64 0           norm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_1_2 (Conv2D)               (None, 6288, 20, 64) 36928       relu_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_a (Conv2D)                 (None, 6288, 20, 64) 2368        wavelet[0][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling2D) (None, 2096, 7, 64)  0           conv_1_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling2D) (None, 2096, 7, 64)  0           conv_a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_1_2 (BatchNormalization)   (None, 2096, 7, 64)  256         max_pooling2d_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "norm_a (BatchNormalization)     (None, 2096, 7, 64)  256         max_pooling2d_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "relu_1_2 (Activation)           (None, 2096, 7, 64)  0           norm_1_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_a (Activation)             (None, 2096, 7, 64)  0           norm_a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 2096, 7, 128) 0           relu_1_2[0][0]                   \n",
      "                                                                 relu_a[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_b (Conv2D)                 (None, 3144, 10, 64) 2368        wavelet[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, 2096, 7, 128) 147584      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling2D) (None, 1048, 4, 64)  0           conv_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_2 (BatchNormalization)     (None, 2096, 7, 128) 512         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_b (BatchNormalization)     (None, 1048, 4, 64)  256         max_pooling2d_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "relu_2 (Activation)             (None, 2096, 7, 128) 0           norm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_b (Activation)             (None, 1048, 4, 64)  0           norm_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_2_2 (Conv2D)               (None, 1048, 4, 128) 147584      relu_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_b_2 (Conv2D)               (None, 1048, 4, 128) 73856       relu_b[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_c (Conv2D)                 (None, 1572, 5, 64)  2368        wavelet[0][3]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling2D) (None, 350, 2, 128)  0           conv_2_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling2D) (None, 350, 2, 128)  0           conv_b_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling2D) (None, 524, 2, 64)   0           conv_c[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_2_2 (BatchNormalization)   (None, 350, 2, 128)  512         max_pooling2d_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "norm_b_2 (BatchNormalization)   (None, 350, 2, 128)  512         max_pooling2d_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "norm_c (BatchNormalization)     (None, 524, 2, 64)   256         max_pooling2d_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "relu_2_2 (Activation)           (None, 350, 2, 128)  0           norm_2_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_b_2 (Activation)           (None, 350, 2, 128)  0           norm_b_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_c (Activation)             (None, 524, 2, 64)   0           norm_c[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 350, 2, 256)  0           relu_2_2[0][0]                   \n",
      "                                                                 relu_b_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_c_2 (Conv2D)               (None, 524, 2, 256)  147712      relu_c[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, 350, 2, 256)  590080      concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling2D) (None, 175, 1, 256)  0           conv_c_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "nomr_3 (BatchNormalization)     (None, 350, 2, 256)  1024        conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_c_2 (BatchNormalization)   (None, 175, 1, 256)  1024        max_pooling2d_40[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "relu_3 (Activation)             (None, 350, 2, 256)  0           nomr_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu_c_2 (Activation)           (None, 175, 1, 256)  0           norm_c_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_3_2 (Conv2D)               (None, 175, 1, 256)  590080      relu_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_c_3 (Conv2D)               (None, 175, 1, 256)  590080      relu_c_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling2D) (None, 59, 1, 256)   0           conv_3_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling2D) (None, 59, 1, 256)   0           conv_c_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_3_2 (BatchNormalization)   (None, 59, 1, 256)   1024        max_pooling2d_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "norm_c_3 (BatchNormalization)   (None, 59, 1, 256)   1024        max_pooling2d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "relu_3_2 (Activation)           (None, 59, 1, 256)   0           norm_3_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_c_3 (Activation)           (None, 59, 1, 256)   0           norm_c_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 59, 1, 512)   0           relu_3_2[0][0]                   \n",
      "                                                                 relu_c_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, 59, 1, 256)   1179904     concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_42 (MaxPooling2D) (None, 20, 1, 256)   0           conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_4 (BatchNormalization)     (None, 20, 1, 256)   1024        max_pooling2d_42[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "relu_4 (Activation)             (None, 20, 1, 256)   0           norm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4_2 (Conv2D)               (None, 10, 1, 256)   590080      relu_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_43 (MaxPooling2D) (None, 4, 1, 256)    0           conv_4_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_4_2 (BatchNormalization)   (None, 4, 1, 256)    1024        max_pooling2d_43[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "relu_4_2 (Activation)           (None, 4, 1, 256)    0           norm_4_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv_5_1 (Conv2D)               (None, 4, 1, 128)    295040      relu_4_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_5_1 (BatchNormalization)   (None, 4, 1, 128)    512         conv_5_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "relu_5_1 (Activation)           (None, 4, 1, 128)    0           norm_5_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool_5_1 (AveragePooling2D) (None, 4, 1, 128)    0           relu_5_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flat_5_1 (Flatten)              (None, 512)          0           avg_pool_5_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "fc_5 (Dense)                    (None, 2048)         1050624     flat_5_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "norm_5 (BatchNormalization)     (None, 2048)         8192        fc_5[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "relu_5 (Activation)             (None, 2048)         0           norm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_5 (Dropout)                (None, 2048)         0           relu_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fc_6 (Dense)                    (None, 2048)         4196352     drop_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "norm_6 (BatchNormalization)     (None, 2048)         8192        fc_6[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "relu_6 (Activation)             (None, 2048)         0           norm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop_6 (Dropout)                (None, 2048)         0           relu_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "fc_7 (Dense)                    (None, 2)            4098        drop_6[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 9,675,970\n",
      "Trainable params: 9,662,882\n",
      "Non-trainable params: 13,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_wavelet_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "id": "w6zpGp6flIEz",
    "outputId": "25b3b88a-b5f9-42d1-c769-8d46517c3bd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0220 20:53:23.745131 140650830956352 module_wrapper.py:137] From /home/rohit/anaconda3/envs/EEG/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chunking(Sequence):\n",
    "    def __init__(self, data_train, train_labels, batch_size):\n",
    "        self.data_train = data_train\n",
    "        self.train_labels = train_labels\n",
    "        self.batch_size = batch_size\n",
    "        self.n = 0\n",
    "        self.max = self.__len__()\n",
    "        self.train_batch = []\n",
    "    def __len__(self):\n",
    "        return np.ceil(int(len(self.data_train) / int(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.data_train[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.train_labels[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "    def __next__(self):\n",
    "        if self.n >= self.max:\n",
    "            self.n = 0\n",
    "        result = self.__getitem__(self.n)\n",
    "        self.n += 1\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section to import labels and trimmed audio of ASVSpoof 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.load(\"/media/rohit/New Volume/codes/SA/backup_hinton/rohit/spoof/spoof/npy_data_asvspoof/train_label.npy\")\n",
    "trimmed_audio = np.load(\"/media/rohit/New Volume/codes/SA/backup_hinton/rohit/spoof/spoof/npy_data_asvspoof/trimmed_audio.npy\", mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=list(train_labels)\n",
    "\n",
    "train_labels1 = list()\n",
    "for i in train_labels:\n",
    "    if i == b'human':\n",
    "        train_labels1.append(1)\n",
    "    else:\n",
    "        train_labels1.append(0)\n",
    "train_labels = to_categorical(train_labels1[:16000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"/media/rohit/New Volume/codes/SA/temp_npy/trimmed_audio_train.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load(\"/media/rohit/New Volume/codes/SA/temp_npy/labels_train.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.load(\"/media/rohit/New Volume/codes/SA/temp_npy/trimmed_audio_val.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = np.load(\"/media/rohit/New Volume/codes/SA/temp_npy/labels_val.npy\",mmap_mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11200, 75673, 1), (4800, 75673, 1))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch_generator = chunking(X_train,y_train,2)\n",
    "validation_batch_generator = chunking(X_val,y_val,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "ejmciqicnMnu",
    "outputId": "ebacd6d9-2162-4215-da3c-b4afca30a00d"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape', (11200, 75673, 1))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9c22b6ecab98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \tbatch_size=32)\n\u001b[0m\u001b[1;32m     17\u001b[0m val_generator = val_data_gen.flow(\n\u001b[1;32m     18\u001b[0m         \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EEG/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py\u001b[0m in \u001b[0;36mflow\u001b[0;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, subset)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0msave_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0msave_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         )\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EEG/lib/python3.6/site-packages/keras_preprocessing/image/numpy_array_iterator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, dtype)\u001b[0m\n\u001b[1;32m    115\u001b[0m             raise ValueError('Input data in `NumpyArrayIterator` '\n\u001b[1;32m    116\u001b[0m                              \u001b[0;34m'should have rank 4. You passed an array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                              'with shape', self.x.shape)\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mchannels_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdata_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'channels_last'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchannels_axis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Input data in `NumpyArrayIterator` should have rank 4. You passed an array with shape', (11200, 75673, 1))"
     ]
    }
   ],
   "source": [
    "train_data_gen = ImageDataGenerator(\n",
    "\t#rescale=1./255,\n",
    "\tshear_range=0.1,\n",
    "\tzoom_range=0.1,\n",
    "\thorizontal_flip=True\n",
    ")\n",
    "val_data_gen = ImageDataGenerator(\n",
    "\t#rescale=1./255\n",
    ")\n",
    "\n",
    "\n",
    "# def train & test generators\n",
    "train_generator = train_data_gen.flow(\n",
    "    X_train,\n",
    "    y_train,\n",
    "\tbatch_size=32)\n",
    "val_generator = val_data_gen.flow(\n",
    "\tX_val,\n",
    "\ty_val,\n",
    "    batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "colab_type": "code",
    "id": "V2740Mdynf0w",
    "outputId": "bfc3b627-135e-436e-ac02-82838f4e7ea6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0220 20:53:35.994663 140650830956352 deprecation.py:323] From /home/rohit/anaconda3/envs/EEG/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1428: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "  14/5600 [..............................] - ETA: 4:29:06 - loss: 6.4439 - acc: 0.5000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a86fa36c84e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_batch_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4800\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \tverbose=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/EEG/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EEG/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EEG/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EEG/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EEG/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EEG/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/EEG/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "\ttraining_batch_generator,\n",
    "\tsteps_per_epoch=11200//2,\n",
    "\tepochs=200,\n",
    "\tvalidation_data=validation_batch_generator,\n",
    "    validation_steps=4800//2,\n",
    "\tverbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "MI3d6ONirH6h",
    "outputId": "2c7ff91a-14dc-46c3-bd45-d2e3d00d7dab"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7e6d1eec5e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(acc, label='training accuracy')\n",
    "plt.plot(val_acc, label='validation accuracy')\n",
    "plt.title('Accuracy curve')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "FjlFssvfrIwO",
    "outputId": "f1bfef36-bc05-4740-fd73-4168d19762ad"
   },
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.plot(loss, label='training loss')\n",
    "plt.plot(val_loss, label='validation loss')\n",
    "plt.title('Loss curve')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(input_shape,out_dim):\n",
    "    #\n",
    "    inputs = Input(input_shape)\n",
    "#     print(inputs)\n",
    "    x = sincnet.SincConv1D(80, 251, 16000)(inputs)\n",
    "\n",
    "\n",
    "    x = MaxPooling1D(pool_size=3)(x)\n",
    "    x = BatchNormalization(momentum=0.05)(x)\n",
    "    x = sincnet.LayerNorm()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    #DNN final\n",
    "    prediction = layers.Dense(out_dim, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=prediction)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 75673, 1)          0         \n",
      "_________________________________________________________________\n",
      "sinc_conv1d_5 (SincConv1D)   (None, 75423, 80)         160       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 25141, 80)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 25141, 80)         320       \n",
      "_________________________________________________________________\n",
      "layer_norm_2 (LayerNorm)     (None, 25141, 80)         160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 25141, 80)         0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25141, 2)          162       \n",
      "=================================================================\n",
      "Total params: 802\n",
      "Trainable params: 642\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input_shape = (n,1)\n",
    "model = getModel((trimmed_audio[1].shape[0],1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75673,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed_audio[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = [1,0,0,0,0,0]\n",
    "do_validation = validation_data!=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(do_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": " waveletCNN_Keras_0.6_[Exp on 10K_data].ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
