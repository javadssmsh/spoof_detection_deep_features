{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36991,
     "status": "ok",
     "timestamp": 1583388674333,
     "user": {
      "displayName": "Rohit Arora",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giutc51X9fjwvH53IqxO8CGT0SR9FudqJV1MJXb=s64",
      "userId": "15885060245314397173"
     },
     "user_tz": -330
    },
    "id": "VnEyfmhu8mN6",
    "outputId": "138da956-98e3-4f10-9b8d-09fe3ae7f5ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1655,
     "status": "ok",
     "timestamp": 1583405007888,
     "user": {
      "displayName": "Rohit Arora",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giutc51X9fjwvH53IqxO8CGT0SR9FudqJV1MJXb=s64",
      "userId": "15885060245314397173"
     },
     "user_tz": -330
    },
    "id": "_PDMjPOPojA0",
    "outputId": "c669e0cf-930b-4016-e556-d0edd74dacb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/SA/Code/spoof_detection_deep_features\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/My Drive/SA/Code/spoof_detection_deep_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Using cached librosa-0.7.2.tar.gz (1.6 MB)\n",
      "Collecting audioread>=2.0.0\n",
      "  Using cached audioread-2.1.8.tar.gz (21 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/rohita/rohit/spoof/work3/lib/python3.5/site-packages (from librosa) (1.18.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/rohita/rohit/spoof/work3/lib/python3.5/site-packages (from librosa) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /home/rohita/rohit/spoof/work3/lib/python3.5/site-packages (from librosa) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.12 in /home/rohita/rohit/spoof/work3/lib/python3.5/site-packages (from librosa) (0.14.1)\n",
      "Requirement already satisfied: decorator>=3.0.0 in /home/rohita/rohit/spoof/work3/lib/python3.5/site-packages (from librosa) (4.4.1)\n",
      "Requirement already satisfied: six>=1.3 in /home/rohita/rohit/spoof/work3/lib/python3.5/site-packages (from librosa) (1.14.0)\n",
      "Collecting resampy>=0.2.2\n",
      "  Using cached resampy-0.2.2.tar.gz (323 kB)\n",
      "Collecting numba>=0.43.0\n",
      "  Downloading numba-0.47.0-cp35-cp35m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 489 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soundfile>=0.9.0\n",
      "  Using cached SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Collecting llvmlite>=0.31.0dev0\n",
      "  Downloading llvmlite-0.31.0-cp35-cp35m-manylinux1_x86_64.whl (20.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.2 MB 126 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/rohita/rohit/spoof/work3/lib/python3.5/site-packages (from numba>=0.43.0->librosa) (45.2.0)\n",
      "Collecting cffi>=1.0\n",
      "  Downloading cffi-1.14.0-cp35-cp35m-manylinux1_x86_64.whl (399 kB)\n",
      "\u001b[K     |████████████████████████████████| 399 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pycparser\n",
      "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
      "\u001b[K     |████████████████████████████████| 112 kB 12.1 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: librosa, audioread, resampy\n",
      "  Building wheel for librosa (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for librosa: filename=librosa-0.7.2-py3-none-any.whl size=1612883 sha256=932ee4a00b1dd0b8d08cf7820cbc471375d3fe809de67ca30581e3cf4ecfec22\n",
      "  Stored in directory: /home/rohita/.cache/pip/wheels/4b/72/78/82101d52cdfc32755f1606294a72d380aabb3fe70e105a71a0\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-2.1.8-py3-none-any.whl size=23091 sha256=29d61d9a3fde15b5aefed5d44a683e06853cf28a006ba1979214be4ceccd738b\n",
      "  Stored in directory: /home/rohita/.cache/pip/wheels/fb/86/30/67eca0131420c96216f3a8a63a69795f6f5ebedad799ab0b00\n",
      "  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320720 sha256=6bbb4bed096205dd44a502aebfb32d93c488e21cf6d1eaf1bbc15eec870bcbed\n",
      "  Stored in directory: /home/rohita/.cache/pip/wheels/8a/37/1c/0a08130bbb9d7c7b7bbab74e4b7e5cd5db50817c225733418c\n",
      "Successfully built librosa audioread resampy\n",
      "Installing collected packages: audioread, llvmlite, numba, resampy, pycparser, cffi, soundfile, librosa\n",
      "Successfully installed audioread-2.1.8 cffi-1.14.0 librosa-0.7.2 llvmlite-0.31.0 numba-0.47.0 pycparser-2.20 resampy-0.2.2 soundfile-0.10.3.post1\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gqIVOEX-osX2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rohita/rohit/spoof/work3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "import numpy as np\n",
    "import librosa as lb\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "# import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "# import soundfile as sf\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "from keras.utils import plot_model,to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# import speechpy as sp\n",
    "# import statistics\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense, Activation, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C85qcuLApQCU"
   },
   "outputs": [],
   "source": [
    "filename = \"/home/rohita/rohit/spoof/ASVspoof2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\"\n",
    "\n",
    "# open the file for reading\n",
    "filehandle = open(filename, 'r')\n",
    "train_protocol = []\n",
    "while True:\n",
    "    # read a single line\n",
    "    line = (filehandle.readline())\n",
    "    train_protocol.append(line)\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "# close the pointer to that file\n",
    "filehandle.close()\n",
    "\n",
    "train_protocol = [s[:-1] for s in train_protocol]\n",
    "\n",
    "train_protocol = pd.DataFrame([s.split(' ') for s in train_protocol])\n",
    "\n",
    "train_protocol.columns = ['speaker_id','file_id', 'blah','system_id', 'label']\n",
    "\n",
    "train_protocol = train_protocol[['speaker_id', 'file_id', 'system_id', 'label']]\n",
    "\n",
    "train_protocol = train_protocol.dropna()\n",
    "\n",
    "train_protocol.drop_duplicates(subset =\"file_id\",keep = 'first',inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# This step was done to create a dataframe to keep track of the speaker id and system id of the samples but due to lack of space i deleted the flac files so i cant do it now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1031,
     "status": "ok",
     "timestamp": 1583405062824,
     "user": {
      "displayName": "Rohit Arora",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giutc51X9fjwvH53IqxO8CGT0SR9FudqJV1MJXb=s64",
      "userId": "15885060245314397173"
     },
     "user_tz": -330
    },
    "id": "nWh-V7NJ-x6M",
    "outputId": "fb4234c0-e91a-41e6-e51e-aee0260a4bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rohita/rohit/spoof/ASVspoof2019/LA/ASVspoof2019_LA_train/flac\n",
      "25380\n"
     ]
    }
   ],
   "source": [
    "#import names of files in dataset\n",
    "path = r'/home/rohita/rohit/spoof/ASVspoof2019/LA/ASVspoof2019_LA_train/flac'\n",
    "files = []\n",
    "missing=[]\n",
    "print(path)\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.flac' in file  :        \n",
    "            files.append(os.path.join(r, file))\n",
    "        else:\n",
    "            missing.append(file)\n",
    "print(len(files))\n",
    "\n",
    "\n",
    "files = [s.split('/') for s in files]\n",
    "\n",
    "files = [s[-1] for s in files]\n",
    "\n",
    "files = [s[:-5] for s in files]\n",
    "\n",
    "files1 = [s.split(' ') for s in files]\n",
    "\n",
    "for s in files:\n",
    "    if len(s)>12:\n",
    "        print(s)\n",
    "        files.remove(s)\n",
    "print(len(files))\n",
    "\n",
    "train_file_id = list(train_protocol.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NOiyu93TSoIW",
    "outputId": "b6651f73-0c1c-4452-f1c1-daeeaf3b7096",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25379\n"
     ]
    }
   ],
   "source": [
    "train_labels = []\n",
    "train_audio = []\n",
    "for count,audio in enumerate(files):\n",
    "    index = train_file_id.index(audio)\n",
    "    if bool(index) == True:\n",
    "        train_audio.append(lb.load('/home/rohita/rohit/spoof/ASVspoof2019/LA/ASVspoof2019_LA_train/flac/'+audio+'.flac',sr=16000))\n",
    "        train_labels.append(train_protocol.iloc[index,3])\n",
    "    if count%100 == 0 :\n",
    "        print(count)\n",
    "        \n",
    "print(len(train_audio))\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train.npy\",np.array(train_audio))\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_labels.npy\",train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n"
     ]
    }
   ],
   "source": [
    "train_protocol_ordered_based_on_files = pd.DataFrame(columns=['speaker_id','file_id','system_id','label'])\n",
    "for count,audio in enumerate(files):\n",
    "    index = train_file_id.index(audio)\n",
    "    if bool(index) == True:\n",
    "        train_protocol_ordered_based_on_files=train_protocol_ordered_based_on_files.append(train_protocol.iloc[index])\n",
    "    if count%100 == 0 :\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_protocol_ordered_based_on_files = train_protocol_ordered_based_on_files.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A01': 3800, 'A06': 3800, 'A02': 3800, '-': 2579, 'A03': 3800, 'A05': 3800, 'A04': 3800}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(train_protocol_ordered_based_on_files.iloc[:,2], return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2012,
     "status": "ok",
     "timestamp": 1583408796773,
     "user": {
      "displayName": "Rohit Arora",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giutc51X9fjwvH53IqxO8CGT0SR9FudqJV1MJXb=s64",
      "userId": "15885060245314397173"
     },
     "user_tz": -330
    },
    "id": "CAt-ukBkgn-q",
    "outputId": "251fef79-7255-4e9f-fdb0-eb4f394243f7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speaker_id         LA_0087\n",
      "file_id       LA_T_4941896\n",
      "system_id              A01\n",
      "label                spoof\n",
      "Name: 4170, dtype: object\n"
     ]
    }
   ],
   "source": [
    "index = train_file_id.index('LA_T_4941896')\n",
    "print(train_protocol.iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1852,
     "status": "ok",
     "timestamp": 1583409719463,
     "user": {
      "displayName": "Rohit Arora",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giutc51X9fjwvH53IqxO8CGT0SR9FudqJV1MJXb=s64",
      "userId": "15885060245314397173"
     },
     "user_tz": -330
    },
    "id": "MDPyiT4YaQRo",
    "outputId": "5a44656a-e94a-4af1-87f1-0bf7e5669207"
   },
   "outputs": [],
   "source": [
    "for count, i in enumerate(train_protocol_ordered_based_on_files.iloc[:,-2]):\n",
    "    if i =='-':\n",
    "        train_protocol_ordered_based_on_files.iloc[count,-2] = 0\n",
    "    elif i == 'A01':\n",
    "        train_protocol_ordered_based_on_files.iloc[count,-2] = 1\n",
    "    elif i == 'A02':\n",
    "        train_protocol_ordered_based_on_files.iloc[count,-2] = 2\n",
    "    elif i == 'A03':\n",
    "        train_protocol_ordered_based_on_files.iloc[count,-2] = 3\n",
    "    elif i == 'A04':\n",
    "        train_protocol_ordered_based_on_files.iloc[count,-2] = 4\n",
    "    elif i == 'A05':\n",
    "        train_protocol_ordered_based_on_files.iloc[count,-2] = 5\n",
    "    elif i == 'A06':\n",
    "        train_protocol_ordered_based_on_files.iloc[count,-2] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4381,
     "status": "ok",
     "timestamp": 1583410049897,
     "user": {
      "displayName": "Rohit Arora",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Giutc51X9fjwvH53IqxO8CGT0SR9FudqJV1MJXb=s64",
      "userId": "15885060245314397173"
     },
     "user_tz": -330
    },
    "id": "qhIUP6ypat63",
    "outputId": "deec8df9-834b-440d-8577-085ed3a7d25a"
   },
   "outputs": [],
   "source": [
    "train_audio.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## load data for splitting the train part of ASVspoof 2019 dataset in train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_labels.npy\")\n",
    "\n",
    "labels=list(labels)\n",
    "\n",
    "labels1 = []\n",
    "for i in labels:\n",
    "    if i == 'bonafide':\n",
    "        labels1.append(1)\n",
    "    else:\n",
    "        labels1.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trim the audio to median length and set human to 1 and spoof to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "audio = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train.npy\",allow_pickle=True)\n",
    "labels = train_protocol_ordered_based_on_files.iloc[:,-1]\n",
    "labels=list(labels)\n",
    "\n",
    "labels1 = []\n",
    "for i in labels:\n",
    "    if i == 'bonafide':\n",
    "        labels1.append(0)\n",
    "    else:\n",
    "        labels1.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "audio = [s[0] for s in audio]\n",
    "\n",
    "audio = np.array(audio)\n",
    "\n",
    "audio_shapes = np.array([s.shape[0] for s in audio])\n",
    "\n",
    "np.median(audio_shapes)\n",
    "\n",
    "trimmed_audio = pad_sequences(audio,maxlen=51222,value=0.0,padding='pre',truncating='post',dtype='float32')\n",
    "\n",
    "trimmed_audio[2]\n",
    "\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train.npy\",trimmed_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## shuffle only the samples and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 22800, 1: 2579}\n",
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.000031  0.000031  0.000031  0.000031  0.000031  0.000031  0.000031   \n",
      "5 -0.000488 -0.000488 -0.000336 -0.000183 -0.000092 -0.000031  0.000214   \n",
      "6 -0.001434 -0.000946 -0.000397 -0.000214 -0.000214 -0.000153  0.000671   \n",
      "7  0.001434  0.000916  0.000549  0.000549  0.000488  0.000061  0.000275   \n",
      "8 -0.002319 -0.002380 -0.002502 -0.002777 -0.002747 -0.002502 -0.002777   \n",
      "9  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "          7         8         9  ...     51213     51214     51215     51216  \\\n",
      "0  0.000000  0.000000  0.000000  ... -0.000061 -0.000061 -0.000031 -0.000122   \n",
      "1  0.000000  0.000000  0.000000  ...  0.000885  0.000885  0.000854  0.000763   \n",
      "2  0.000000  0.000000  0.000000  ... -0.007446 -0.009033 -0.007812 -0.009125   \n",
      "3  0.000000  0.000000  0.000000  ...  0.000153  0.000183  0.000183  0.000183   \n",
      "4  0.000031  0.000031  0.000031  ... -0.000122 -0.000092 -0.000183 -0.000153   \n",
      "5  0.000427  0.000214  0.000031  ... -0.000397 -0.000336 -0.000397 -0.000458   \n",
      "6  0.001038  0.000946  0.000793  ... -0.024567  0.019379 -0.006165 -0.018158   \n",
      "7  0.000092 -0.000793 -0.001434  ... -0.000549 -0.000519 -0.000641 -0.000580   \n",
      "8 -0.003082 -0.003357 -0.003174  ...  0.000153  0.000671 -0.000031  0.000610   \n",
      "9  0.000000  0.000000  0.000000  ...  0.000122  0.000092  0.000122  0.000122   \n",
      "\n",
      "      51217     51218     51219     51220     51221  label  \n",
      "0 -0.000061 -0.000092 -0.000031 -0.000061 -0.000031      0  \n",
      "1  0.000916  0.000732  0.000671  0.000763  0.000641      0  \n",
      "2 -0.010406 -0.012299 -0.013306 -0.014893 -0.014771      0  \n",
      "3  0.000183  0.000183  0.000183  0.000183  0.000183      0  \n",
      "4 -0.000153 -0.000153 -0.000122 -0.000153 -0.000153      0  \n",
      "5 -0.000275 -0.000153 -0.000061 -0.000061 -0.000031      0  \n",
      "6  0.005737  0.003174 -0.002441  0.006714 -0.014221      0  \n",
      "7 -0.000427 -0.000488 -0.000519 -0.000427 -0.000458      0  \n",
      "8  0.000916  0.001984  0.001404  0.001129  0.001404      0  \n",
      "9  0.000092  0.000092  0.000092  0.000092  0.000061      0  \n",
      "\n",
      "[10 rows x 51223 columns]\n",
      "(23211, 51223)\n",
      "(46011, 51223)\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(labels1, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "dataset = pd.DataFrame(audio)\n",
    "dataset['label'] = labels1\n",
    "print(dataset.head(10))\n",
    "\n",
    "\n",
    "shuffled_df = dataset.sample(frac=1,random_state=4)\n",
    "\n",
    "# Put all the human class in a separate dataset.\n",
    "human_samples = shuffled_df.loc[shuffled_df['label'] == 0]\n",
    "human_samples = human_samples.append([human_samples]*8,ignore_index=True)\n",
    "print(human_samples.shape)\n",
    "#Randomly select 3674 observations from the spoofed (majority class)\n",
    "spoofed_samples = shuffled_df.loc[shuffled_df['label'] == 0].sample(frac=1,random_state=42)\n",
    "\n",
    "# Concatenate both dataframes again\n",
    "normalized_df = pd.concat([human_samples, spoofed_samples])\n",
    "\n",
    "normalized_df = normalized_df.sample(frac=1,random_state=42)\n",
    "\n",
    "print(normalized_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(normalized_df.iloc[:32208,:51221])\n",
    "y_train = np.array(normalized_df.iloc[:32208,-1])\n",
    "\n",
    "X_val = np.array(normalized_df.iloc[32208:,:51221])\n",
    "y_val = np.array(normalized_df.iloc[32208:,-1])\n",
    "\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_train.npy\",X_train)\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_train_labels.npy\",y_train)\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_val.npy\",X_val)\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_val_labels.npy\",y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## shuffle and make proper dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spoof': 22800, 'bonafide': 2579}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(train_protocol_ordered_based_on_files.iloc[:,3], return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "dataset = pd.DataFrame(audio)\n",
    "dataset['speaker_id'] = train_protocol_ordered_based_on_files['speaker_id']\n",
    "dataset['file_id'] = train_protocol_ordered_based_on_files['file_id']\n",
    "dataset['system_id'] = train_protocol_ordered_based_on_files['system_id']\n",
    "dataset['label'] = [0 if s == 'bonafide' else 1 for s in train_protocol_ordered_based_on_files.iloc[:,-1]]\n",
    "\n",
    "# print(dataset.head(10))\n",
    "\n",
    "\n",
    "shuffled_df = dataset.sample(frac=1,random_state=4)\n",
    "\n",
    "# Put all the human class in a separate dataset.\n",
    "human_samples = shuffled_df.loc[shuffled_df['system_id'] == 0]\n",
    "# human_samples = human_samples.append(human_samples,ignore_index=True)\n",
    "# human_samples = human_samples.sample(frac=1,random_state =4)\n",
    "\n",
    "#Randomly select 3674 observations from the spoofed (majority class)\n",
    "spoofed_samples = shuffled_df.loc[shuffled_df['system_id'] == 2].sample(frac=1,random_state=42)\n",
    "\n",
    "# Concatenate both dataframes again\n",
    "normalized_df = pd.concat([human_samples, spoofed_samples])\n",
    "del train_protocol_ordered_based_on_files\n",
    "del audio\n",
    "del dataset\n",
    "del shuffled_df\n",
    "del human_samples\n",
    "del spoofed_samples\n",
    "normalized_df = normalized_df.sample(frac=1,random_state=42)\n",
    "# normalized_df = normalized_df.drop_duplicates(keep='first',inplace = True)\n",
    "# print(normalized_df.shape)\n",
    "\n",
    "# normalized_df['audio'] = normalized_df[normalized_df.columns[0:51222]].values.tolist()\n",
    "# # print(normalized_df.head(10))\n",
    "# normalized_df.drop(normalized_df.iloc[:, 0:51222], inplace = True, axis = 1) \n",
    "# # print(normalized_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# normalized_df.to_pickle(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_df.pkl\")\n",
    "X_train = np.array(normalized_df.iloc[:,:-4])\n",
    "y_train = np.array(normalized_df.iloc[:,-2])\n",
    "normalized_df = normalized_df.iloc[:,-4:]\n",
    "normalized_df.to_pickle(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_df_sub.pkl\")\n",
    "\n",
    "# X_val = np.array(normalized_df.iloc[17766:,:51221])\n",
    "# y_val = np.array(normalized_df.iloc[17766:,-2])\n",
    "\n",
    "\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_sub.npy\",X_train)\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_sub_labels.npy\",y_train)\n",
    "# np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_val_sid.npy\",X_val)\n",
    "# np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_train_val_sid_labels.npy\",y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25379"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for count,i in enumerate(labels1):\n",
    "#     print(i,train_protocol_ordered_based_on_files.iloc[count,-1])\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/rohita/rohit/spoof/ASVspoof2019/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\"\n",
    "\n",
    "# open the file for reading\n",
    "filehandle = open(filename, 'r')\n",
    "dev_protocol = []\n",
    "while True:\n",
    "    # read a single line\n",
    "    line = (filehandle.readline())\n",
    "    dev_protocol.append(line)\n",
    "    if not line:\n",
    "        break\n",
    "\n",
    "# close the pointer to that file\n",
    "filehandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_protocol = [s[:-1] for s in dev_protocol]\n",
    "\n",
    "dev_protocol = pd.DataFrame([s.split(' ') for s in dev_protocol])\n",
    "\n",
    "dev_protocol.columns = ['speaker_id','file_id', 'blah','system_id', 'label']\n",
    "\n",
    "dev_protocol = dev_protocol[['speaker_id', 'file_id', 'system_id', 'label']]\n",
    "\n",
    "dev_protocol = dev_protocol.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rohita/rohit/spoof/ASVspoof2019/LA/ASVspoof2019_LA_dev/flac\n",
      "24986\n",
      "LA_D_A2940120\n",
      "LA_D_A8082083\n",
      "LA_D_A9165322\n",
      "LA_D_A9063912\n",
      "LA_D_A6763914\n",
      "LA_D_A8486194\n",
      "LA_D_A5086574\n",
      "LA_D_A7944567\n",
      "LA_D_A2004586\n",
      "LA_D_A2486210\n",
      "LA_D_A1045602\n",
      "LA_D_A8631378\n",
      "LA_D_A9163645\n",
      "LA_D_A5637003\n",
      "LA_D_A2300370\n",
      "LA_D_A1640883\n",
      "LA_D_A2113518\n",
      "LA_D_A5452066\n",
      "LA_D_A2667946\n",
      "LA_D_A8555382\n",
      "LA_D_A4397042\n",
      "LA_D_A9282295\n",
      "LA_D_A7526873\n",
      "LA_D_A5781599\n",
      "LA_D_A4372259\n",
      "LA_D_A7495873\n",
      "LA_D_A9442421\n",
      "LA_D_A5058238\n",
      "LA_D_A4011660\n",
      "LA_D_A9738230\n",
      "LA_D_A8630807\n",
      "LA_D_A1600475\n",
      "LA_D_A6997796\n",
      "LA_D_A6934713\n",
      "LA_D_A2091569\n",
      "LA_D_A9509678\n",
      "LA_D_A5053798\n",
      "LA_D_A1146342\n",
      "LA_D_A7770316\n",
      "LA_D_A9567385\n",
      "LA_D_A5088604\n",
      "LA_D_A6866194\n",
      "LA_D_A9832811\n",
      "LA_D_A8202532\n",
      "LA_D_A7729162\n",
      "LA_D_A4464508\n",
      "LA_D_A1981362\n",
      "LA_D_A1305336\n",
      "LA_D_A3079793\n",
      "LA_D_A3322786\n",
      "LA_D_A9497856\n",
      "LA_D_A4129936\n",
      "LA_D_A7005427\n",
      "LA_D_A2936391\n",
      "LA_D_A4645563\n",
      "LA_D_A8203727\n",
      "LA_D_A6583126\n",
      "LA_D_A5688215\n",
      "LA_D_A3276106\n",
      "LA_D_A3333968\n",
      "LA_D_A9087858\n",
      "LA_D_A9474465\n",
      "LA_D_A8551139\n",
      "LA_D_A9939030\n",
      "LA_D_A8133373\n",
      "LA_D_A6270180\n",
      "LA_D_A5113711\n",
      "LA_D_A8977595\n",
      "LA_D_A6600208\n",
      "LA_D_A1858303\n",
      "LA_D_A2665626\n",
      "LA_D_A3406323\n",
      "LA_D_A7993459\n",
      "LA_D_A3973482\n",
      "LA_D_A4570689\n",
      "LA_D_A7798936\n",
      "LA_D_A3263738\n",
      "LA_D_A2400074\n",
      "LA_D_A6845047\n",
      "LA_D_A9454316\n",
      "LA_D_A2379320\n",
      "LA_D_A1339494\n",
      "LA_D_A1519783\n",
      "LA_D_A3321758\n",
      "LA_D_A4381979\n",
      "LA_D_A9785066\n",
      "LA_D_A4618181\n",
      "LA_D_A4237204\n",
      "LA_D_A4526472\n",
      "LA_D_A3209304\n",
      "LA_D_A1839254\n",
      "LA_D_A1921351\n",
      "LA_D_A6433167\n",
      "LA_D_A7567703\n",
      "LA_D_A4442634\n",
      "LA_D_A8565895\n",
      "LA_D_A3607255\n",
      "LA_D_A5839814\n",
      "LA_D_A4315761\n",
      "LA_D_A9921674\n",
      "LA_D_A5078465\n",
      "LA_D_A1510649\n",
      "LA_D_A2491384\n",
      "LA_D_A7702408\n",
      "LA_D_A2172149\n",
      "LA_D_A9722933\n",
      "LA_D_A6778708\n",
      "LA_D_A1911418\n",
      "LA_D_A4107355\n",
      "LA_D_A3666159\n",
      "LA_D_A5087707\n",
      "LA_D_A9941952\n",
      "LA_D_A7558995\n",
      "LA_D_A7491692\n",
      "LA_D_A3536231\n",
      "LA_D_A7339881\n",
      "LA_D_A1425742\n",
      "LA_D_A1225110\n",
      "LA_D_A2481400\n",
      "LA_D_A3337879\n",
      "LA_D_A1046944\n",
      "LA_D_A2156760\n",
      "LA_D_A7884506\n",
      "LA_D_A6092926\n",
      "LA_D_A8724031\n",
      "LA_D_A8486849\n",
      "LA_D_A6999220\n",
      "LA_D_A4537573\n",
      "LA_D_A2531416\n",
      "LA_D_A4417954\n",
      "LA_D_A6141181\n",
      "LA_D_A5572316\n",
      "LA_D_A3068483\n",
      "LA_D_A3849768\n",
      "LA_D_A5692827\n",
      "LA_D_A9089748\n",
      "LA_D_A5248955\n",
      "LA_D_A3191299\n",
      "LA_D_A8477585\n",
      "LA_D_A4159128\n",
      "LA_D_A7448192\n",
      "LA_D_A4907836\n"
     ]
    }
   ],
   "source": [
    "#import names of files in dataset\n",
    "path = r'/home/rohita/rohit/spoof/ASVspoof2019/LA/ASVspoof2019_LA_dev/flac'\n",
    "files = []\n",
    "missing=[]\n",
    "print(path)\n",
    "for r, d, f in os.walk(path):\n",
    "    for file in f:\n",
    "        if '.flac' in file  :        \n",
    "            files.append(os.path.join(r, file))\n",
    "        else:\n",
    "            missing.append(file)\n",
    "print(len(files))\n",
    "\n",
    "\n",
    "files = [s.split('/') for s in files]\n",
    "\n",
    "files = [s[-1] for s in files]\n",
    "\n",
    "files = [s[:-5] for s in files]\n",
    "\n",
    "files1 = [s.split(' ') for s in files]\n",
    "\n",
    "for s in files:\n",
    "    if len(s)>12:\n",
    "        print(s)\n",
    "        files.remove(s)\n",
    "\n",
    "dev_file_id = list(dev_protocol.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n"
     ]
    }
   ],
   "source": [
    "dev_labels = []\n",
    "dev_audio = []\n",
    "for count,audio in enumerate(files):\n",
    "    index = dev_file_id.index(audio)\n",
    "    if bool(index) == True:\n",
    "        dev_audio.append(lb.load('/home/rohita/rohit/spoof/ASVspoof2019/LA/ASVspoof2019_LA_dev/flac/'+audio+'.flac',sr=16000))\n",
    "        dev_labels.append(dev_protocol.iloc[index,3])\n",
    "    if count%100 == 0 :\n",
    "        print(count)\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev.npy\",np.array(dev_audio))\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_labels.npy\",dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## if you want to run this the dev data is already prepared "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n"
     ]
    }
   ],
   "source": [
    "dev_protocol_ordered_based_on_files = pd.DataFrame(columns=['speaker_id','file_id','system_id','label'])\n",
    "for count,audio in enumerate(files):\n",
    "    index = dev_file_id.index(audio)\n",
    "    if bool(index) == True:\n",
    "        dev_protocol_ordered_based_on_files=dev_protocol_ordered_based_on_files.append(dev_protocol.iloc[index])\n",
    "    if count%100 == 0 :\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_protocol_ordered_based_on_files = dev_protocol_ordered_based_on_files.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_protocol_ordered_based_on_files['label'] = [0 if s == 'bonafide' else 1 for s in dev_protocol_ordered_based_on_files.iloc[:,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_protocol_ordered_based_on_files.to_pickle(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "audio = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev.npy\", allow_pickle=True)\n",
    "# labels = np.load(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_labels.npy\")\n",
    "# labels=list(labels)\n",
    "\n",
    "# labels1 = []\n",
    "# for i in labels:\n",
    "#     if i == 'bonafide':\n",
    "#         labels1.append(1)\n",
    "#     else:\n",
    "#         labels1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = [s[0] for s in audio]\n",
    "\n",
    "audio = np.array(audio)\n",
    "\n",
    "audio_shapes = np.array([s.shape[0] for s in audio])\n",
    "\n",
    "np.median(audio_shapes)\n",
    "\n",
    "trimmed_audio = pad_sequences(audio,maxlen=51222,value=0.0,padding='pre',truncating='post',dtype='float32')\n",
    "\n",
    "trimmed_audio[2]\n",
    "\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev.npy\",trimmed_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spoof': 22296, 'bonafide': 2547}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(dev_protocol_ordered_based_on_files.iloc[:,3], return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A03': 3716, 'A05': 3716, '-': 2547, 'A02': 3716, 'A01': 3716, 'A06': 3716, 'A04': 3716}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(dev_protocol_ordered_based_on_files.iloc[:,2], return_counts=True)\n",
    "print(dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, i in enumerate(dev_protocol_ordered_based_on_files.iloc[:,-2]):\n",
    "    if i =='-':\n",
    "        dev_protocol_ordered_based_on_files.iloc[count,-2] = 0\n",
    "    elif i == 'A01':\n",
    "        dev_protocol_ordered_based_on_files.iloc[count,-2] = 1\n",
    "    elif i == 'A02':\n",
    "        dev_protocol_ordered_based_on_files.iloc[count,-2] = 2\n",
    "    elif i == 'A03':\n",
    "        dev_protocol_ordered_based_on_files.iloc[count,-2] = 3\n",
    "    elif i == 'A04':\n",
    "        dev_protocol_ordered_based_on_files.iloc[count,-2] = 4\n",
    "    elif i == 'A05':\n",
    "        dev_protocol_ordered_based_on_files.iloc[count,-2] = 5\n",
    "    elif i == 'A06':\n",
    "        dev_protocol_ordered_based_on_files.iloc[count,-2] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0      1 speaker_id  \\\n",
      "0  [-0.00091552734, -0.0023498535, -0.0043945312,...  16000    LA_0071   \n",
      "1  [3.0517578e-05, 3.0517578e-05, 3.0517578e-05, ...  16000    LA_0071   \n",
      "2  [3.0517578e-05, 3.0517578e-05, 3.0517578e-05, ...  16000    LA_0073   \n",
      "3  [-0.0022277832, -0.002166748, -0.0022277832, -...  16000    LA_0074   \n",
      "4  [-0.0032348633, -0.0030822754, -0.0033569336, ...  16000    LA_0072   \n",
      "5  [-0.0039978027, -0.0036621094, -0.0034484863, ...  16000    LA_0077   \n",
      "6  [-0.00021362305, 0.00018310547, 0.0019226074, ...  16000    LA_0070   \n",
      "7  [6.1035156e-05, 6.1035156e-05, 6.1035156e-05, ...  16000    LA_0075   \n",
      "8  [-0.00064086914, -0.001373291, -0.0024719238, ...  16000    LA_0102   \n",
      "9  [0.0023498535, 0.002319336, 0.0026245117, 0.00...  16000    LA_0072   \n",
      "\n",
      "        file_id system_id  label  \n",
      "0  LA_D_9616032         5      1  \n",
      "1  LA_D_4264656         2      1  \n",
      "2  LA_D_6273410         4      1  \n",
      "3  LA_D_6947992         5      1  \n",
      "4  LA_D_9410329         6      1  \n",
      "5  LA_D_3624723         5      1  \n",
      "6  LA_D_2591082         2      1  \n",
      "7  LA_D_5223993         3      1  \n",
      "8  LA_D_7153183         0      0  \n",
      "9  LA_D_3428449         1      1  \n"
     ]
    }
   ],
   "source": [
    "# unique, counts = np.unique(dev_protocol_ordered_based_on_files.iloc[:,3], return_counts=True)\n",
    "# print(dict(zip(unique, counts)))\n",
    "\n",
    "dataset = pd.DataFrame(audio)\n",
    "dataset['speaker_id'] = dev_protocol_ordered_based_on_files['speaker_id']\n",
    "dataset['file_id'] = dev_protocol_ordered_based_on_files['file_id']\n",
    "dataset['system_id'] = dev_protocol_ordered_based_on_files['system_id']\n",
    "dataset['label'] = [0 if s == 'bonafide' else 1 for s in dev_protocol_ordered_based_on_files.iloc[:,-1]]\n",
    "\n",
    "print(dataset.head(10))\n",
    "\n",
    "\n",
    "shuffled_df = dataset.sample(frac=1,random_state=4)\n",
    "\n",
    "# Put all the human class in a separate dataset.\n",
    "human_samples = shuffled_df.loc[shuffled_df['system_id'] == 0]\n",
    "# human_samples = human_samples.append(human_samples,ignore_index=True)\n",
    "# human_samples = human_samples.sample(frac=1,random_state =4)\n",
    "\n",
    "#Randomly select 3674 observations from the spoofed (majority class)\n",
    "spoofed_samples = shuffled_df.loc[shuffled_df['system_id'] == 2].sample(frac=1,random_state=42)\n",
    "\n",
    "# Concatenate both dataframes again\n",
    "normalized_df = pd.concat([human_samples, spoofed_samples])\n",
    "del dev_protocol_ordered_based_on_files\n",
    "del audio\n",
    "del dataset\n",
    "del shuffled_df\n",
    "del human_samples\n",
    "del spoofed_samples\n",
    "normalized_df = normalized_df.sample(frac=1,random_state=42)\n",
    "\n",
    "X_train = np.array(normalized_df.iloc[:,:-4])\n",
    "y_train = np.array(normalized_df.iloc[:,-2])\n",
    "normalized_df = normalized_df.iloc[:,-4:]\n",
    "normalized_df.to_pickle(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_df_sub.pkl\")\n",
    "\n",
    "\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_sub.npy\",X_train)\n",
    "np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_sub_labels.npy\",y_train)\n",
    "# np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_val.npy\",X_val)\n",
    "# np.save(\"/home/rohita/rohit/spoof/npy_data_asvspoof/ASVspoof2019_dev_val_labels.npy\",y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPmWvZNdxtkESkWtgpkIvi7",
   "collapsed_sections": [],
   "name": "ASVspoof2019_data_prepration.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}